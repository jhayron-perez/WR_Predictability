{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57f07e14-a574-436a-8966-21cc745f87de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/glade/work/jhayron/conda-envs/pytorch_wr/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import copy\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import brier_score_loss\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import sys\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import glob \n",
    "\n",
    "\n",
    "from sklearn import datasets, ensemble\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.utils import class_weight\n",
    "import json\n",
    "\n",
    "import xgboost as xgb\n",
    "from bayes_opt import BayesianOptimization\n",
    "from bayes_opt import acquisition\n",
    "import shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb3c9cec-d4fa-4a69-a115-3d4baef3df4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_val_test_periods(full_df):\n",
    "    dic_train_val = {}\n",
    "    dic_test = {}\n",
    "    \n",
    "    start_of_test_periods = np.arange(1981,2021,10)\n",
    "    end_of_test_periods = start_of_test_periods + 9\n",
    "    \n",
    "    for iperiod in range(len(start_of_test_periods)):\n",
    "        df_test_temp = full_df[str(start_of_test_periods[iperiod]):str(end_of_test_periods[iperiod])]\n",
    "        df_trainval_temp = full_df.drop(df_test_temp.index)\n",
    "        \n",
    "        dic_train_val[start_of_test_periods[iperiod]] = df_trainval_temp\n",
    "        dic_test[start_of_test_periods[iperiod]] = df_test_temp\n",
    "    return dic_train_val, dic_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e281a0f-ef51-4fa2-a673-bd5ece361b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_random_forecast(df_week_0, seed_value=42):\n",
    "    # Set the random seed for reproducibility\n",
    "    np.random.seed(seed_value)\n",
    "    \n",
    "    # Step 1: Get unique classes and their frequencies\n",
    "    values = df_week_0[df_week_0.keys()[0]].value_counts()\n",
    "    \n",
    "    # Step 2: Calculate the probabilities for each class\n",
    "    classes = values.index  # Unique classes\n",
    "    probabilities = values / values.sum()  # Normalize to get probability distribution\n",
    "    \n",
    "    # Step 3: Generate a random forecast based on the probabilities\n",
    "    random_forecast = np.random.choice(classes, size=len(df_week_0), p=probabilities)\n",
    "    \n",
    "    # Step 4: Return the random forecast as a DataFrame or Series\n",
    "    forecast_df = pd.DataFrame(random_forecast, index=df_week_0.index, columns=['y_predicted'])\n",
    "    \n",
    "    return forecast_df\n",
    "\n",
    "def generate_random_forecast_probabilities(df_week_0, seed_value=42):\n",
    "    # Set the random seed for reproducibility\n",
    "    np.random.seed(seed_value)\n",
    "    # Step 1: Get unique classes and their frequencies\n",
    "    values = df_week_0[df_week_0.keys()[0]].value_counts()\n",
    "    \n",
    "    # Step 2: Calculate the probabilities for each class\n",
    "    classes = values.index  # Unique classes\n",
    "    probabilities = values / values.sum()  # Normalize to get probability distribution\n",
    "    \n",
    "    # Step 3: Create a probability forecast for each sample\n",
    "    # Create a 2D array where each row is the same probability distribution\n",
    "    prob_matrix = np.tile(probabilities.values, (len(df_week_0), 1))\n",
    "    \n",
    "    # Step 4: Return the probability matrix as a DataFrame\n",
    "    forecast_df = pd.DataFrame(prob_matrix, index=df_week_0.index, columns=classes)[np.arange(len(classes))]\n",
    "    \n",
    "    return forecast_df\n",
    "\n",
    "def generate_random_forecast_with_monthly_probabilities(df_week_0, seed_value=42):\n",
    "    # Set the random seed for reproducibility\n",
    "    np.random.seed(seed_value)\n",
    "    \n",
    "    # Extract the month from the index (assuming the index is a datetime index)\n",
    "    df_week_0['month'] = df_week_0.index.month\n",
    "    \n",
    "    # Prepare an empty list to store the random forecast\n",
    "    forecasts = []\n",
    "    \n",
    "    # Loop through each month\n",
    "    for month in range(1, 13):  # Loop through months 1 to 12\n",
    "        # Filter data for the current month\n",
    "        month_data = df_week_0[df_week_0['month'] == month]\n",
    "        \n",
    "        # Step 1: Get unique classes and their frequencies for the current month\n",
    "        values = month_data[df_week_0.keys()[0]].value_counts()\n",
    "        \n",
    "        # Step 2: Calculate the probabilities for each class in the current month\n",
    "        classes = values.index  # Unique classes\n",
    "        probabilities = values / values.sum()  # Normalize to get probability distribution\n",
    "        \n",
    "        # Step 3: Generate random forecasts for the current month based on the probabilities\n",
    "        month_forecast = np.random.choice(classes, size=len(month_data), p=probabilities)\n",
    "        \n",
    "        # Store the forecast for the current month\n",
    "        forecasts.append(pd.Series(month_forecast, index=month_data.index))\n",
    "    \n",
    "    # Combine all monthly forecasts into one DataFrame\n",
    "    forecast_df = pd.concat(forecasts)\n",
    "    forecast_df = forecast_df.sort_index()  # Sort the index to preserve the original order\n",
    "    forecast_df = pd.DataFrame(forecast_df,columns=['y_predicted'])\n",
    "    return forecast_df\n",
    "    \n",
    "def generate_probability_forecast_with_monthly_probabilities(df_week_0, seed_value=42):\n",
    "    # Set the random seed for reproducibility\n",
    "    np.random.seed(seed_value)\n",
    "    \n",
    "    # Extract the month from the index (assuming the index is a datetime index)\n",
    "    df_week_0['month'] = df_week_0.index.month\n",
    "    \n",
    "    # Prepare an empty DataFrame to store the probability forecasts\n",
    "    all_probabilities = pd.DataFrame(index=df_week_0.index)\n",
    "    \n",
    "    # Loop through each month\n",
    "    for month in range(1, 13):  # Loop through months 1 to 12\n",
    "        # Filter data for the current month\n",
    "        month_data = df_week_0[df_week_0['month'] == month]\n",
    "        \n",
    "        if month_data.empty:\n",
    "            continue  # Skip if there's no data for the month\n",
    "        \n",
    "        # Step 1: Get unique classes and their frequencies for the current month\n",
    "        values = month_data[df_week_0.keys()[0]].value_counts()\n",
    "        \n",
    "        # Step 2: Calculate the probabilities for each class in the current month\n",
    "        classes = values.index  # Unique classes\n",
    "        probabilities = values / values.sum()  # Normalize to get probability distribution\n",
    "        \n",
    "        # Step 3: Create a probability matrix for the current month\n",
    "        prob_matrix = np.tile(probabilities.values, (len(month_data), 1))\n",
    "        \n",
    "        # Create a DataFrame for this month's probabilities with appropriate columns\n",
    "        month_prob_df = pd.DataFrame(prob_matrix, index=month_data.index, columns=classes)\n",
    "        \n",
    "        # Append this month's DataFrame to the overall probability DataFrame\n",
    "        all_probabilities = pd.concat([all_probabilities, month_prob_df])\n",
    "    \n",
    "    # Sort the index to match the original order\n",
    "    all_probabilities = all_probabilities.sort_index()\n",
    "    \n",
    "    # Fill missing columns with zeros for months that do not include certain classes\n",
    "    all_classes = df_week_0[df_week_0.keys()[0]].unique()\n",
    "    all_probabilities = all_probabilities.reindex(columns=all_classes, fill_value=0).dropna()[np.arange(len(classes))]\n",
    "    \n",
    "    return all_probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b95adc65-cecf-4d7f-a03e-882d8e0a7d2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Dec 25 20:51:37 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA A100-SXM4-80GB          On  |   00000000:81:00.0 Off |                    0 |\n",
      "| N/A   32C    P0             61W /  500W |       9MiB /  81920MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   1  NVIDIA A100-SXM4-80GB          On  |   00000000:C1:00.0 Off |                    0 |\n",
      "| N/A   39C    P0            195W /  500W |    6443MiB /  81920MiB |     91%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    1   N/A  N/A    250523      C   ...on/conda-envs/pytorch_wr/bin/python       6430MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Run nvidia-smi to get GPU information\n",
    "os.system('nvidia-smi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "985e9f29-bd3c-4895-9951-70eae5cd5d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_id = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f75fd80-d939-4806-ae4e-86b5d4126fc5",
   "metadata": {},
   "source": [
    "# Explore ranges weeks 6 and 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "32b43f9e-ccba-41f0-801d-def9ea133dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "wr_original_series = pd.read_csv('/glade/work/jhayron/Data4Predictability/WR_Series_vSLtest.csv',\\\n",
    "                index_col=0,names=['week0','dist'],skiprows=1,parse_dates=True)\n",
    "# Rolling window for mode\n",
    "rolling_mode = (\n",
    "    wr_original_series.rolling('7d', center=True,min_periods=7)\n",
    "    .apply(lambda x: x.mode()[0] if not x.mode().empty else float('nan'))\n",
    ").shift(-3)\n",
    "\n",
    "# Rolling window for the count of the mode\n",
    "rolling_mode_count = (\n",
    "    wr_original_series.rolling('7d', center=True,min_periods=7)\n",
    "    .apply(lambda x: (x == x.mode()[0]).sum() if not x.mode().empty else 0)\n",
    ").shift(-3)\n",
    "\n",
    "# If duration of WR during week was less than 4, assing NO WR class\n",
    "rolling_mode.loc[rolling_mode_count['week0']<4,'week0'] = 4\n",
    "wr_series_mode = copy.deepcopy(rolling_mode)\n",
    "time_index = pd.to_datetime(wr_series_mode.index).dayofweek\n",
    "wr_series_mode = wr_series_mode.iloc[time_index.isin([0,3])].dropna()\n",
    "wr_series = copy.deepcopy(wr_series_mode)\n",
    "\n",
    "for wk in range(2,10):\n",
    "    series_temp = copy.deepcopy(wr_series[\"week0\"])\n",
    "    series_temp.index = series_temp.index - timedelta(weeks = wk-1)\n",
    "    series_temp.name = f'week{wk-1}'\n",
    "    if wk==2:\n",
    "        df_shifts = pd.concat([pd.DataFrame(wr_series[\"week0\"]),pd.DataFrame(series_temp)],axis=1)  \n",
    "    else:\n",
    "        df_shifts = pd.concat([df_shifts,pd.DataFrame(series_temp)],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df31837f-ee68-4963-aa15-ba6105327818",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_files_anoms = np.sort(glob.glob('/glade/derecho/scratch/jhayron/Data4Predictability/WeeklyAnoms_DetrendedStd_v3_2dg/*.nc'))\n",
    "list_vars = [list_files_anoms[i].split('/')[-1][:-3] for i in range(len(list_files_anoms))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1c6986db-8d27-4fae-b601-f1cd2bc848fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 IC_SODA\n",
      "1 IT_SODA\n",
      "2 MLD_SODA\n",
      "3 OHC100_SODA\n",
      "4 OHC200_SODA\n",
      "5 OHC300_SODA\n",
      "6 OHC50_SODA\n",
      "7 OHC700_SODA\n",
      "8 OLR_ERA5\n",
      "9 SD_ERA5\n",
      "10 SSH_SODA\n",
      "11 SST_OISSTv2\n",
      "12 SST_SODA\n",
      "13 STL_1m_ERA5\n",
      "14 STL_28cm_ERA5\n",
      "15 STL_7cm_ERA5\n",
      "16 STL_full_ERA5\n",
      "17 SWVL_1m_ERA5\n",
      "18 SWVL_28cm_ERA5\n",
      "19 SWVL_7cm_ERA5\n",
      "20 SWVL_full_ERA5\n",
      "21 U10_ERA5\n",
      "22 U200_ERA5\n",
      "23 Z500_ERA5\n"
     ]
    }
   ],
   "source": [
    "for ivar,var in enumerate(list_vars):\n",
    "    print(ivar,var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5e5eee4d-9471-4a5f-832b-34e122246ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ivar = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0370a443-4c2d-494f-b4db-a7600fe70184",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_eval(preds, dtrain):\n",
    "    labels = dtrain.get_label()\n",
    "    preds = np.argmax(preds, axis=1)  # Get the predicted class\n",
    "    f1 = f1_score(labels, preds, average='micro')\n",
    "    return 'f1_eval', f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "782b8b10-29e2-411f-9fed-e64f4ba0315a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OHC100_SODA\n"
     ]
    }
   ],
   "source": [
    "print(list_vars[ivar])\n",
    "\n",
    "path_weekly_anoms = '/glade/derecho/scratch/jhayron/Data4Predictability/WeeklyAnoms_DetrendedStd_v3_2dg/'\n",
    "path_nc_anoms = f'{path_weekly_anoms}{list_vars[ivar]}.nc'\n",
    "anoms = xr.open_dataset(path_nc_anoms)\n",
    "anoms = anoms.assign_coords(time=pd.DatetimeIndex(anoms.time).normalize())\n",
    "var_name_nc = list(anoms.data_vars.keys())[0]\n",
    "\n",
    "# Define a boxcar filter function\n",
    "def boxcar_filter(data, size):\n",
    "    kernel = np.ones((size, size)) / (size * size)\n",
    "    from scipy.signal import convolve2d\n",
    "    return convolve2d(data, kernel, mode=\"same\", boundary=\"fill\", fillvalue=np.nan)\n",
    "\n",
    "# Apply boxcar filter\n",
    "smoothed_anoms = xr.apply_ufunc(\n",
    "    boxcar_filter,\n",
    "    anoms,\n",
    "    kwargs={\"size\": 3},  # Adjust window size (e.g., 5x5 grid cells)\n",
    "    input_core_dims=[[\"lat\", \"lon\"]],\n",
    "    output_core_dims=[[\"lat\", \"lon\"]],\n",
    "    vectorize=True,\n",
    ")\n",
    "\n",
    "anoms_flattened = smoothed_anoms[var_name_nc].stack(flat_spatial=('lat', 'lon'))\n",
    "anoms_flattened_og = copy.deepcopy(anoms_flattened)\n",
    "# anoms_flattened_og.data[:,anoms_flattened.columns] = anoms_flattened.values WITH THIS LINES I CAN COME BACK\n",
    "# anoms_flattened_og.unstack('flat_spatial') WITH THIS LINES I CAN COME BACK\n",
    "anoms_flattened = pd.DataFrame(anoms_flattened,index = anoms_flattened.time)\n",
    "anoms_flattened = anoms_flattened.dropna(axis=1, how='any')\n",
    "\n",
    "combined_df = copy.deepcopy(anoms_flattened)\n",
    "combined_df['day_sin'] = np.sin(2 * np.pi * combined_df.index.day_of_year / 365)\n",
    "combined_df['day_cos'] = np.cos(2 * np.pi * combined_df.index.day_of_year / 365)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dfc5606b-d7dc-43ed-beb0-be83449d295d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WEEK: 3\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/glade/work/jhayron/conda-envs/pytorch_wr/lib/python3.12/site-packages/xgboost/core.py:158: UserWarning: [20:53:14] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1730232887822/work/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
      "Potential solutions:\n",
      "- Use a data structure that matches the device ordinal in the booster.\n",
      "- Set the device for booster before call to inplace_predict.\n",
      "\n",
      "This warning will only be shown once.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.17641418983700863\n",
      "1\n",
      "0.25311601150527324\n",
      "2\n",
      "0.22413793103448276\n",
      "3\n",
      "0.19922630560928434\n",
      "**** Micro results ****\n",
      "F1 XGBoost: 0.2132564841498559\n",
      "F1 Random: 0.20292987512007685\n",
      "F1 Persistence: 0.22478386167146974\n",
      "F1 Climatology: 0.21805955811719502\n",
      "**** Macro results ****\n",
      "F1 XGBoost: 0.20601898701039528\n",
      "F1 Random: 0.19645202516507373\n",
      "F1 Persistence: 0.21859992915184262\n",
      "F1 Climatology: 0.21241307411836083\n",
      "**** Frequencies ****\n",
      "True distribution:\n",
      "[0.25336215 0.2240634  0.19164265 0.15465898 0.17627281]\n",
      "Forecasted distribution:\n",
      "[0.25216138 0.2660903  0.19116234 0.15682037 0.13376561]\n",
      "WEEK: 6\n",
      "0\n",
      "0.23873441994247363\n",
      "1\n",
      "0.24544582933844677\n",
      "2\n",
      "0.22126436781609196\n",
      "3\n",
      "0.23689320388349513\n",
      "**** Micro results ****\n",
      "F1 XGBoost: 0.23557692307692307\n",
      "F1 Random: 0.203125\n",
      "F1 Persistence: 0.21322115384615384\n",
      "F1 Climatology: 0.20721153846153847\n",
      "**** Macro results ****\n",
      "F1 XGBoost: 0.21617468756924235\n",
      "F1 Random: 0.19664654104730958\n",
      "F1 Persistence: 0.20614731171892373\n",
      "F1 Climatology: 0.20120935238977591\n",
      "**** Frequencies ****\n",
      "True distribution:\n",
      "[0.25336538 0.22427885 0.19182692 0.15432692 0.17620192]\n",
      "Forecasted distribution:\n",
      "[0.25456731 0.31706731 0.18653846 0.12115385 0.12067308]\n"
     ]
    }
   ],
   "source": [
    "f1s_test2 = []\n",
    "f1s_random2 = []\n",
    "f1s_persistence2 = []\n",
    "f1s_climatology2 = []\n",
    "\n",
    "# for week_out in range(0,9):\n",
    "for week_out in [3,6]:\n",
    "    print(f'WEEK: {week_out}')\n",
    "    week_out_str = f'week{week_out}'\n",
    "\n",
    "    fully_combined_df = pd.concat([combined_df,df_shifts[week_out_str]],axis=1)\n",
    "    fully_combined_df = fully_combined_df.dropna()\n",
    "\n",
    "    dic_trainval, dic_test = get_train_val_test_periods(fully_combined_df)\n",
    "    start_of_test_periods = np.arange(1981,2021,10)\n",
    "    \n",
    "    df_week_forecast = df_shifts[[week_out_str]].dropna()\n",
    "    \n",
    "    random_forecast = generate_random_forecast(df_week_forecast,\n",
    "                                               seed_value=42)\n",
    "    climatology_forecast = generate_random_forecast_with_monthly_probabilities(df_week_forecast, \n",
    "                                                                               seed_value=42)\n",
    "    random_forecast_probs = generate_random_forecast_probabilities(df_week_forecast)\n",
    "    climatology_forecast_probs = generate_probability_forecast_with_monthly_probabilities(df_week_forecast)\n",
    "\n",
    "    list_results = []\n",
    "    list_results_probs = []\n",
    "    \n",
    "    for iperiod in range(len(start_of_test_periods)):\n",
    "        print(iperiod)\n",
    "        X_trainval = dic_trainval[start_of_test_periods[iperiod]].iloc[:,:-1].values\n",
    "        y_trainval = dic_trainval[start_of_test_periods[iperiod]].iloc[:,-1]\n",
    "        \n",
    "        X_test = dic_test[start_of_test_periods[iperiod]].iloc[:,:-1].values\n",
    "        y_test = dic_test[start_of_test_periods[iperiod]].iloc[:,-1]\n",
    "        #### HERE DEFINITION OF THE MODEL ####\n",
    "        hyperparams = {\n",
    "            \"n_estimators\": 20,       # Number of boosting rounds, balances speed and depth\n",
    "            \"max_depth\": 3,            # Controls tree depth; deeper trees model complexity better but risk overfitting\n",
    "            \"learning_rate\": 10**(-1),      # Step size shrinkage for reducing overfitting risk\n",
    "            \"subsample\": 0.85,          # Fraction of training samples used per tree, helps generalize\n",
    "            \"colsample_bytree\": 0.7,   # Fraction of features used per tree, improves robustness\n",
    "            \"colsample_bylevel\": 1,   # Fraction of features used per tree, improves robustness\n",
    "            \"gamma\": 2.5,                # Minimum loss reduction to make a split, prevents overfitting\n",
    "            \"min_child_weight\": 10,     # Minimum sum of instance weight needed in a leaf, controls complexity\n",
    "            \"reg_alpha\": 10**(1.1),            # L1 regularization term for weights, often fine at 0 unless high sparsity is needed\n",
    "            \"reg_lambda\": 10**(2)            # L2 regularization term for weights, controls overfitting\n",
    "        }\n",
    "        cw = class_weight.compute_sample_weight(\n",
    "            class_weight='balanced',\n",
    "            y=y_trainval\n",
    "        )\n",
    "        cw = cw**0.8\n",
    "        model = xgb.XGBClassifier(n_estimators=hyperparams['n_estimators'],\n",
    "                            max_depth=hyperparams['max_depth'],\n",
    "                            learning_rate=hyperparams['learning_rate'],\n",
    "                            subsample=hyperparams['subsample'],\n",
    "                            colsample_bytree=hyperparams['colsample_bytree'],\n",
    "                            colsample_bylevel=hyperparams['colsample_bylevel'],\n",
    "                            gamma=hyperparams['gamma'],\n",
    "                            reg_alpha=hyperparams['reg_alpha'],\n",
    "                            reg_lambda=hyperparams['reg_lambda'],\n",
    "                            num_class=5,\n",
    "                            objective = \"multi:softprob\",\n",
    "                            tree_method='hist',\n",
    "                            device = f'cuda:{gpu_id}')\n",
    "        model.fit(X_trainval, y_trainval, sample_weight=cw)\n",
    "        y_predicted = model.predict(X_test)\n",
    "        print(f1_score(y_test,y_predicted,average='micro'))\n",
    "        y_predicted_probs = model.predict_proba(X_test)\n",
    "        y_predicted_probs = pd.DataFrame(y_predicted_probs,index=y_test.index)\n",
    "        df_results_temp = pd.DataFrame(np.array([y_test.values,y_predicted]).T,\n",
    "                                       index=y_test.index,\n",
    "                                       columns=['y_true','y_predicted'])\n",
    "        list_results.append(df_results_temp)\n",
    "        list_results_probs.append(y_predicted_probs)\n",
    "        \n",
    "    df_results_full = pd.concat(list_results,axis=0)\n",
    "    df_results_probs_full = pd.concat(list_results_probs,axis=0)\n",
    "    print('**** Micro results ****')\n",
    "    f1_results = f1_score(df_results_full['y_true'],df_results_full['y_predicted'],average='micro')\n",
    "    \n",
    "    f1_random = f1_score(df_results_full['y_true'],\n",
    "             random_forecast['y_predicted'].loc[df_results_full['y_true'].index],average='micro')\n",
    "    df_week_0 = df_shifts[['week0']].dropna()\n",
    "    persistence_forecast = df_week_0.loc[df_results_full['y_true'].index]\n",
    "    f1_persistence = f1_score(df_results_full['y_true'],\n",
    "             persistence_forecast['week0'].loc[df_results_full['y_true'].index],average='micro')\n",
    "    \n",
    "    f1_climatology = f1_score(df_results_full['y_true'],\n",
    "             climatology_forecast['y_predicted'].loc[df_results_full['y_true'].index],average='micro')\n",
    "    print(f'F1 XGBoost:', f1_results)\n",
    "    print('F1 Random:',f1_random)\n",
    "    print('F1 Persistence:',f1_persistence)\n",
    "    print('F1 Climatology:',f1_climatology)\n",
    "    f1s_test2.append(f1_results)\n",
    "    f1s_random2.append(f1_random)\n",
    "    f1s_persistence2.append(f1_persistence)\n",
    "    f1s_climatology2.append(f1_climatology)\n",
    "    print(f'F1 XGBoost:', f1_results)\n",
    "    print('F1 Random:',f1_random)\n",
    "    print('F1 Persistence:',f1_persistence)\n",
    "    print('F1 Climatology:',f1_climatology)\n",
    "    print('**** Frequencies ****')\n",
    "    print('True distribution:')\n",
    "    print(np.bincount(df_results_full['y_true'])/np.sum(np.bincount(df_results_full['y_true'])))\n",
    "    print('Forecasted distribution:')\n",
    "    print(np.bincount(df_results_full['y_predicted'])/np.sum(np.bincount(df_results_full['y_predicted'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea4e5b2-9789-48d1-b4c8-7dd75350b513",
   "metadata": {},
   "source": [
    "# do hyperparameter search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c1f9050b-1119-41f6-9a79-889206bbe128",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_xgboost(X_trainval,y_trainval,path_save = None):\n",
    "    ## Apply Bayesian optimization to XGBoost parameters\n",
    "\n",
    "    def crossval_xgboost(max_depth,\n",
    "                         log10_learning_rate,\n",
    "                         subsample,\n",
    "                         colsample_bytree,\n",
    "                         colsample_bylevel,\n",
    "                         gamma,\n",
    "                         min_child_weight,\n",
    "                         log10_reg_alpha,\n",
    "                         log10_reg_lambda,\n",
    "                         beta_class_weights):\n",
    "        \n",
    "        max_depth = int(max_depth)\n",
    "        learning_rate = 10 ** log10_learning_rate\n",
    "        reg_alpha = 10 ** log10_reg_alpha\n",
    "        reg_lambda = 10 ** log10_reg_lambda\n",
    "        \n",
    "        # Instantiate the XGBoost model\n",
    "        clf = xgb.XGBClassifier(\n",
    "            max_depth=max_depth,\n",
    "            learning_rate=learning_rate,\n",
    "            subsample=subsample,\n",
    "            colsample_bytree=colsample_bytree,\n",
    "            colsample_bylevel=colsample_bylevel,\n",
    "            gamma=gamma,\n",
    "            min_child_weight = min_child_weight,\n",
    "            reg_alpha=reg_alpha,\n",
    "            reg_lambda=reg_lambda,\n",
    "            num_class=5,\n",
    "            device=f'cuda:{gpu_id}',\n",
    "            tree_method='hist',\n",
    "            objective='multi:softprob',\n",
    "            random_state=42\n",
    "        )\n",
    "        \n",
    "        dic_params_cv = clf.get_xgb_params()\n",
    "        \n",
    "        # Custom cross-validation with TimeSeriesSplit\n",
    "        tscv = TimeSeriesSplit(n_splits=3)  # Adjust number of splits as needed\n",
    "        scores = []\n",
    "        for train_index, test_index in tscv.split(X_trainval):\n",
    "            X_train, X_test = X_trainval[train_index], X_trainval[test_index]\n",
    "            y_train, y_test = y_trainval.iloc[train_index], y_trainval.iloc[test_index]\n",
    "\n",
    "            class_weights_arr = compute_class_weight('balanced', \n",
    "                                                     classes=np.unique(y_train), y=y_train)\n",
    "            class_weight_dict = dict(zip(np.unique(y_train), class_weights_arr))\n",
    "            train_weight = np.array([class_weight_dict[label] for label in y_train])**beta_class_weights\n",
    "            \n",
    "            dtrain = xgb.DMatrix(X_train, y_train, weight=train_weight)\n",
    "            dtest = xgb.DMatrix(X_test, y_test)\n",
    "            \n",
    "            # Train the model with early stopping\n",
    "            clf = xgb.train(\n",
    "                    dic_params_cv,\n",
    "                    dtrain,\n",
    "                    num_boost_round=20  # Use the best boosting rounds\n",
    "                )\n",
    "            # Predict and evaluate\n",
    "            preds = clf.predict(dtest)\n",
    "            ###### WITH F1 SCORE ########\n",
    "            score = f1_score(y_test, np.argmax(preds, axis=1), average='micro')  # Use your chosen metric\n",
    "            if (np.bincount(np.argmax(preds, axis=1))\\\n",
    "                /np.sum(np.bincount(np.argmax(preds, axis=1)))).max() > 0.4:\n",
    "                score = score * 0.5\n",
    "            scores.append(score)\n",
    "        # print(scores)\n",
    "        # print(np.mean(scores))\n",
    "        return np.mean(scores)\n",
    "\n",
    "    pbounds = {\n",
    "        # Tree-specific hyperparameters\n",
    "        'max_depth': (2, 20),  # Moderate depth to prevent overfitting\n",
    "        'min_child_weight': (1, 20),  # Prevent overly small leaves\n",
    "        'subsample': (0.7, 0.9),  # Balance between under- and over-sampling\n",
    "        'colsample_bytree': (0.6, 0.9),  # Use a subset of features to reduce variance\n",
    "        'colsample_bylevel': (0.75, 1),  # Similar to colsample_bytree but at each split\n",
    "    \n",
    "        # Learning task-specific hyperparameters\n",
    "        'log10_learning_rate': (-4, -1),  # Learning rate in log10 space to explore lower values\n",
    "        'gamma': (0, 5),  # Regularization term to prevent over-complex trees\n",
    "        'log10_reg_lambda': (0, 2.5),  # L2 regularization\n",
    "        'log10_reg_alpha': (0.6, 1.6),  # L1 regularization\n",
    "    \n",
    "        # General\n",
    "        'beta_class_weights': (0, 2.5),  # Use class weights if needed for imbalanced data\n",
    "    }\n",
    "    \n",
    "    # acq = acquisition.UpperConfidenceBound(kappa=0.1)\n",
    "    acq = acquisition.ExpectedImprovement(xi=0.) ## CHOSEN ONE xi->0 full exploitation\n",
    "    # acq = acquisition.ProbabilityOfImprovement(xi=1e-4)\n",
    "    # acq = GreedyAcquisition(random_state=42)\n",
    "    optimizer = BayesianOptimization(\n",
    "        f=crossval_xgboost,\n",
    "        pbounds=pbounds,\n",
    "        random_state=42,\n",
    "        verbose=1,\n",
    "        acquisition_function=acq)\n",
    "    \n",
    "    # optimizer.maximize(init_points=10, n_iter=50)\n",
    "    optimizer.maximize(\n",
    "        init_points=50,\n",
    "        n_iter=20,\n",
    "        # acq='pi'  # Options: 'ei', 'pi', 'ucb'\n",
    "    )\n",
    "    \n",
    "    best_params = optimizer.max['params']\n",
    "    if path_save:\n",
    "        results_df = pd.DataFrame(optimizer.res)\n",
    "        params_df = pd.json_normalize(results_df['params'])\n",
    "        final_df = pd.concat([params_df, results_df['target']], axis=1)\n",
    "        final_df.to_csv(path_save)\n",
    "    return best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b564c6b3-7516-4485-9eee-e575535d0939",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WEEK: 3\n",
      "0\n",
      "|   iter    |  target   | beta_c... | colsam... | colsam... |   gamma   | log10_... | log10_... | log10_... | max_depth | min_ch... | subsample |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "| \u001b[35m7        \u001b[39m | \u001b[35m0.169    \u001b[39m | \u001b[35m0.9717   \u001b[39m | \u001b[35m0.8178   \u001b[39m | \u001b[35m0.8486   \u001b[39m | \u001b[35m1.784    \u001b[39m | \u001b[35m-3.157   \u001b[39m | \u001b[35m1.143    \u001b[39m | \u001b[35m0.3523   \u001b[39m | \u001b[35m16.44    \u001b[39m | \u001b[35m2.416    \u001b[39m | \u001b[35m0.8974   \u001b[39m |\n",
      "| \u001b[35m14       \u001b[39m | \u001b[35m0.1991   \u001b[39m | \u001b[35m1.044    \u001b[39m | \u001b[35m0.8055   \u001b[39m | \u001b[35m0.636    \u001b[39m | \u001b[35m1.688    \u001b[39m | \u001b[35m-1.171   \u001b[39m | \u001b[35m0.9232   \u001b[39m | \u001b[35m1.297    \u001b[39m | \u001b[35m14.65    \u001b[39m | \u001b[35m7.909    \u001b[39m | \u001b[35m0.8944   \u001b[39m |\n",
      "| \u001b[35m34       \u001b[39m | \u001b[35m0.206    \u001b[39m | \u001b[35m0.8899   \u001b[39m | \u001b[35m0.9395   \u001b[39m | \u001b[35m0.6043   \u001b[39m | \u001b[35m0.5804   \u001b[39m | \u001b[35m-3.862   \u001b[39m | \u001b[35m0.6407   \u001b[39m | \u001b[35m2.139    \u001b[39m | \u001b[35m14.67    \u001b[39m | \u001b[35m10.01    \u001b[39m | \u001b[35m0.7196   \u001b[39m |\n",
      "=================================================================================================================================================\n",
      "0.18120805369127516\n",
      "1\n",
      "|   iter    |  target   | beta_c... | colsam... | colsam... |   gamma   | log10_... | log10_... | log10_... | max_depth | min_ch... | subsample |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "| \u001b[35m42       \u001b[39m | \u001b[35m0.2346   \u001b[39m | \u001b[35m0.7983   \u001b[39m | \u001b[35m0.9875   \u001b[39m | \u001b[35m0.8852   \u001b[39m | \u001b[35m2.867    \u001b[39m | \u001b[35m-2.104   \u001b[39m | \u001b[35m1.048    \u001b[39m | \u001b[35m0.733    \u001b[39m | \u001b[35m7.916    \u001b[39m | \u001b[35m13.78    \u001b[39m | \u001b[35m0.8505   \u001b[39m |\n",
      "=================================================================================================================================================\n",
      "0.22435282837967402\n",
      "2\n",
      "|   iter    |  target   | beta_c... | colsam... | colsam... |   gamma   | log10_... | log10_... | log10_... | max_depth | min_ch... | subsample |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "| \u001b[35m22       \u001b[39m | \u001b[35m0.2316   \u001b[39m | \u001b[35m1.372    \u001b[39m | \u001b[35m0.923    \u001b[39m | \u001b[35m0.7956   \u001b[39m | \u001b[35m1.121    \u001b[39m | \u001b[35m-1.863   \u001b[39m | \u001b[35m0.8372   \u001b[39m | \u001b[35m0.8135   \u001b[39m | \u001b[35m15.44    \u001b[39m | \u001b[35m13.34    \u001b[39m | \u001b[35m0.8698   \u001b[39m |\n",
      "=================================================================================================================================================\n",
      "0.21743295019157088\n",
      "3\n",
      "|   iter    |  target   | beta_c... | colsam... | colsam... |   gamma   | log10_... | log10_... | log10_... | max_depth | min_ch... | subsample |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "| \u001b[35m7        \u001b[39m | \u001b[35m0.2413   \u001b[39m | \u001b[35m0.9717   \u001b[39m | \u001b[35m0.8178   \u001b[39m | \u001b[35m0.8486   \u001b[39m | \u001b[35m1.784    \u001b[39m | \u001b[35m-3.157   \u001b[39m | \u001b[35m1.143    \u001b[39m | \u001b[35m0.3523   \u001b[39m | \u001b[35m16.44    \u001b[39m | \u001b[35m2.416    \u001b[39m | \u001b[35m0.8974   \u001b[39m |\n",
      "=================================================================================================================================================\n",
      "0.19052224371373308\n",
      "**** Micro results ****\n",
      "F1 XGBoost: 0.20341018251681076\n",
      "F1 Random: 0.20292987512007685\n",
      "F1 Persistence: 0.22478386167146974\n",
      "F1 Climatology: 0.21805955811719502\n",
      "**** Macro results ****\n",
      "F1 XGBoost: 0.19926293701102446\n",
      "F1 Random: 0.19645202516507373\n",
      "F1 Persistence: 0.21859992915184262\n",
      "F1 Climatology: 0.21241307411836083\n",
      "**** Frequencies ****\n",
      "True distribution:\n",
      "[0.25336215 0.2240634  0.19164265 0.15465898 0.17627281]\n",
      "Forecasted distribution:\n",
      "[0.21589817 0.22502402 0.19548511 0.15393852 0.20965418]\n",
      "---------------> Running Time: 20.65  minutes.\n",
      "WEEK: 6\n",
      "0\n",
      "|   iter    |  target   | beta_c... | colsam... | colsam... |   gamma   | log10_... | log10_... | log10_... | max_depth | min_ch... | subsample |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "| \u001b[35m7        \u001b[39m | \u001b[35m0.1532   \u001b[39m | \u001b[35m0.9717   \u001b[39m | \u001b[35m0.8178   \u001b[39m | \u001b[35m0.8486   \u001b[39m | \u001b[35m1.784    \u001b[39m | \u001b[35m-3.157   \u001b[39m | \u001b[35m1.143    \u001b[39m | \u001b[35m0.3523   \u001b[39m | \u001b[35m16.44    \u001b[39m | \u001b[35m2.416    \u001b[39m | \u001b[35m0.8974   \u001b[39m |\n",
      "| \u001b[35m12       \u001b[39m | \u001b[35m0.1662   \u001b[39m | \u001b[35m0.7244   \u001b[39m | \u001b[35m0.7903   \u001b[39m | \u001b[35m0.8789   \u001b[39m | \u001b[35m4.041    \u001b[39m | \u001b[35m-2.1     \u001b[39m | \u001b[35m1.471    \u001b[39m | \u001b[35m2.009    \u001b[39m | \u001b[35m5.358    \u001b[39m | \u001b[35m17.96    \u001b[39m | \u001b[35m0.8079   \u001b[39m |\n",
      "| \u001b[35m14       \u001b[39m | \u001b[35m0.1701   \u001b[39m | \u001b[35m1.044    \u001b[39m | \u001b[35m0.8055   \u001b[39m | \u001b[35m0.636    \u001b[39m | \u001b[35m1.688    \u001b[39m | \u001b[35m-1.171   \u001b[39m | \u001b[35m0.9232   \u001b[39m | \u001b[35m1.297    \u001b[39m | \u001b[35m14.65    \u001b[39m | \u001b[35m7.909    \u001b[39m | \u001b[35m0.8944   \u001b[39m |\n",
      "| \u001b[35m21       \u001b[39m | \u001b[35m0.2122   \u001b[39m | \u001b[35m1.605    \u001b[39m | \u001b[35m0.771    \u001b[39m | \u001b[35m0.6485   \u001b[39m | \u001b[35m4.493    \u001b[39m | \u001b[35m-2.181   \u001b[39m | \u001b[35m0.6092   \u001b[39m | \u001b[35m0.2537   \u001b[39m | \u001b[35m13.94    \u001b[39m | \u001b[35m1.096    \u001b[39m | \u001b[35m0.7322   \u001b[39m |\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 37\u001b[0m\n\u001b[1;32m     35\u001b[0m X_test \u001b[38;5;241m=\u001b[39m dic_test[start_of_test_periods[iperiod]]\u001b[38;5;241m.\u001b[39miloc[:,:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues\n\u001b[1;32m     36\u001b[0m y_test \u001b[38;5;241m=\u001b[39m dic_test[start_of_test_periods[iperiod]]\u001b[38;5;241m.\u001b[39miloc[:,\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m---> 37\u001b[0m best_params \u001b[38;5;241m=\u001b[39m \u001b[43moptimize_xgboost\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_trainval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m       \u001b[49m\u001b[43my_trainval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m       \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mResultsTests_Hyperparams_5Classes/df_hyperparams_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlist_vars\u001b[49m\u001b[43m[\u001b[49m\u001b[43mivar\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mweek_out_str\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43miperiod\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mResultsTests_Hyperparams_5Classes/besthyperparams_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlist_vars[ivar]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mweek_out_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00miperiod\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.json\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m json_file:\n\u001b[1;32m     41\u001b[0m     json\u001b[38;5;241m.\u001b[39mdump(best_params, json_file)\n",
      "Cell \u001b[0;32mIn[13], line 103\u001b[0m, in \u001b[0;36moptimize_xgboost\u001b[0;34m(X_trainval, y_trainval, path_save)\u001b[0m\n\u001b[1;32m     95\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m BayesianOptimization(\n\u001b[1;32m     96\u001b[0m     f\u001b[38;5;241m=\u001b[39mcrossval_xgboost,\n\u001b[1;32m     97\u001b[0m     pbounds\u001b[38;5;241m=\u001b[39mpbounds,\n\u001b[1;32m     98\u001b[0m     random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m,\n\u001b[1;32m     99\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m    100\u001b[0m     acquisition_function\u001b[38;5;241m=\u001b[39macq)\n\u001b[1;32m    102\u001b[0m \u001b[38;5;66;03m# optimizer.maximize(init_points=10, n_iter=50)\u001b[39;00m\n\u001b[0;32m--> 103\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[43m    \u001b[49m\u001b[43minit_points\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# acq='pi'  # Options: 'ei', 'pi', 'ucb'\u001b[39;49;00m\n\u001b[1;32m    107\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    109\u001b[0m best_params \u001b[38;5;241m=\u001b[39m optimizer\u001b[38;5;241m.\u001b[39mmax[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m path_save:\n",
      "File \u001b[0;32m/glade/work/jhayron/conda-envs/pytorch_wr/lib/python3.12/site-packages/bayes_opt/bayesian_optimization.py:312\u001b[0m, in \u001b[0;36mBayesianOptimization.maximize\u001b[0;34m(self, init_points, n_iter)\u001b[0m\n\u001b[1;32m    310\u001b[0m     x_probe \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msuggest()\n\u001b[1;32m    311\u001b[0m     iteration \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 312\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprobe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_probe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlazy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bounds_transformer \u001b[38;5;129;01mand\u001b[39;00m iteration \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    315\u001b[0m     \u001b[38;5;66;03m# The bounds transformer should only modify the bounds after\u001b[39;00m\n\u001b[1;32m    316\u001b[0m     \u001b[38;5;66;03m# the init_points points (only for the true iterations)\u001b[39;00m\n\u001b[1;32m    317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_bounds(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bounds_transformer\u001b[38;5;241m.\u001b[39mtransform(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_space))\n",
      "File \u001b[0;32m/glade/work/jhayron/conda-envs/pytorch_wr/lib/python3.12/site-packages/bayes_opt/bayesian_optimization.py:245\u001b[0m, in \u001b[0;36mBayesianOptimization.probe\u001b[0;34m(self, params, lazy)\u001b[0m\n\u001b[1;32m    243\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_queue\u001b[38;5;241m.\u001b[39mappend(params)\n\u001b[1;32m    244\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 245\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_space\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprobe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    246\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch(Events\u001b[38;5;241m.\u001b[39mOPTIMIZATION_STEP)\n",
      "File \u001b[0;32m/glade/work/jhayron/conda-envs/pytorch_wr/lib/python3.12/site-packages/bayes_opt/target_space.py:418\u001b[0m, in \u001b[0;36mTargetSpace.probe\u001b[0;34m(self, params)\u001b[0m\n\u001b[1;32m    416\u001b[0m     error_msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo target function has been provided.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    417\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(error_msg)\n\u001b[0;32m--> 418\u001b[0m target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdict_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    420\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_constraint \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    421\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mregister(x, target)\n",
      "Cell \u001b[0;32mIn[13], line 56\u001b[0m, in \u001b[0;36moptimize_xgboost.<locals>.crossval_xgboost\u001b[0;34m(max_depth, log10_learning_rate, subsample, colsample_bytree, colsample_bylevel, gamma, min_child_weight, log10_reg_alpha, log10_reg_lambda, beta_class_weights)\u001b[0m\n\u001b[1;32m     53\u001b[0m dtest \u001b[38;5;241m=\u001b[39m xgb\u001b[38;5;241m.\u001b[39mDMatrix(X_test, y_test)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# Train the model with early stopping\u001b[39;00m\n\u001b[0;32m---> 56\u001b[0m clf \u001b[38;5;241m=\u001b[39m \u001b[43mxgb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdic_params_cv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_boost_round\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Use the best boosting rounds\u001b[39;49;00m\n\u001b[1;32m     60\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m# Predict and evaluate\u001b[39;00m\n\u001b[1;32m     62\u001b[0m preds \u001b[38;5;241m=\u001b[39m clf\u001b[38;5;241m.\u001b[39mpredict(dtest)\n",
      "File \u001b[0;32m/glade/work/jhayron/conda-envs/pytorch_wr/lib/python3.12/site-packages/xgboost/core.py:726\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    724\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[1;32m    725\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[0;32m--> 726\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/glade/work/jhayron/conda-envs/pytorch_wr/lib/python3.12/site-packages/xgboost/training.py:181\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cb_container\u001b[38;5;241m.\u001b[39mbefore_iteration(bst, i, dtrain, evals):\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m--> 181\u001b[0m \u001b[43mbst\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miteration\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cb_container\u001b[38;5;241m.\u001b[39mafter_iteration(bst, i, dtrain, evals):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/glade/work/jhayron/conda-envs/pytorch_wr/lib/python3.12/site-packages/xgboost/core.py:2101\u001b[0m, in \u001b[0;36mBooster.update\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m   2097\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assign_dmatrix_features(dtrain)\n\u001b[1;32m   2099\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fobj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2100\u001b[0m     _check_call(\n\u001b[0;32m-> 2101\u001b[0m         \u001b[43m_LIB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mXGBoosterUpdateOneIter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2102\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_int\u001b[49m\u001b[43m(\u001b[49m\u001b[43miteration\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\n\u001b[1;32m   2103\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2104\u001b[0m     )\n\u001b[1;32m   2105\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2106\u001b[0m     pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(dtrain, output_margin\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "f1s_test2 = []\n",
    "f1s_random2 = []\n",
    "f1s_persistence2 = []\n",
    "f1s_climatology2 = []\n",
    "\n",
    "# for week_out in range(0,9):\n",
    "for week_out in [3,6]:\n",
    "    start_time = datetime.now()\n",
    "    print(f'WEEK: {week_out}')\n",
    "    week_out_str = f'week{week_out}'\n",
    "\n",
    "    fully_combined_df = pd.concat([combined_df,df_shifts[week_out_str]],axis=1)\n",
    "    fully_combined_df = fully_combined_df.dropna()\n",
    "\n",
    "    dic_trainval, dic_test = get_train_val_test_periods(fully_combined_df)\n",
    "    start_of_test_periods = np.arange(1981,2021,10)\n",
    "    \n",
    "    df_week_forecast = df_shifts[[week_out_str]].dropna()\n",
    "    \n",
    "    random_forecast = generate_random_forecast(df_week_forecast,\n",
    "                                               seed_value=42)\n",
    "    climatology_forecast = generate_random_forecast_with_monthly_probabilities(df_week_forecast, \n",
    "                                                                               seed_value=42)\n",
    "    random_forecast_probs = generate_random_forecast_probabilities(df_week_forecast)\n",
    "    climatology_forecast_probs = generate_probability_forecast_with_monthly_probabilities(df_week_forecast)\n",
    "\n",
    "    list_results = []\n",
    "    list_results_probs = []\n",
    "    \n",
    "    for iperiod in range(len(start_of_test_periods)):\n",
    "        print(iperiod)\n",
    "        X_trainval = dic_trainval[start_of_test_periods[iperiod]].iloc[:,:-1].values\n",
    "        y_trainval = dic_trainval[start_of_test_periods[iperiod]].iloc[:,-1]\n",
    "        \n",
    "        X_test = dic_test[start_of_test_periods[iperiod]].iloc[:,:-1].values\n",
    "        y_test = dic_test[start_of_test_periods[iperiod]].iloc[:,-1]\n",
    "        best_params = optimize_xgboost(X_trainval,\n",
    "               y_trainval,\n",
    "               f'ResultsTests_Hyperparams_5Classes/df_hyperparams_{list_vars[ivar]}_{week_out_str}_{iperiod}.csv')\n",
    "        with open(f'ResultsTests_Hyperparams_5Classes/besthyperparams_{list_vars[ivar]}_{week_out_str}_{iperiod}.json', 'w') as json_file:\n",
    "            json.dump(best_params, json_file)\n",
    "            \n",
    "        #### HERE DEFINITION OF THE MODEL ####\n",
    "        \n",
    "        cw = class_weight.compute_sample_weight(\n",
    "            class_weight='balanced',\n",
    "            y=y_trainval\n",
    "        )\n",
    "        cw = cw**best_params['beta_class_weights']\n",
    "        \n",
    "        model = xgb.XGBClassifier(n_estimators=20,\n",
    "                            max_depth=int(best_params['max_depth']),\n",
    "                            learning_rate=10**best_params['log10_learning_rate'],\n",
    "                            subsample=best_params['subsample'],\n",
    "                            colsample_bytree=best_params['colsample_bytree'],\n",
    "                            colsample_bylevel=best_params['colsample_bylevel'],\n",
    "                            gamma=best_params['gamma'],\n",
    "                            reg_alpha=10**best_params['log10_reg_alpha'],\n",
    "                            reg_lambda=10**best_params['log10_reg_lambda'],\n",
    "                            num_class=5,\n",
    "                            objective = \"multi:softprob\",\n",
    "                            tree_method='hist',\n",
    "                            device = f'cuda:{gpu_id}')\n",
    "        \n",
    "        model.fit(X_trainval, y_trainval, sample_weight=cw)\n",
    "        y_predicted = model.predict(X_test)\n",
    "        print(f1_score(y_test,y_predicted,average='micro'))\n",
    "        y_predicted_probs = model.predict_proba(X_test)\n",
    "        y_predicted_probs = pd.DataFrame(y_predicted_probs,index=y_test.index)\n",
    "        df_results_temp = pd.DataFrame(np.array([y_test.values,y_predicted]).T,\n",
    "                                       index=y_test.index,\n",
    "                                       columns=['y_true','y_predicted'])\n",
    "        list_results.append(df_results_temp)\n",
    "        list_results_probs.append(y_predicted_probs)\n",
    "        \n",
    "    df_results_full = pd.concat(list_results,axis=0)\n",
    "    df_results_probs_full = pd.concat(list_results_probs,axis=0)\n",
    "    print('**** Micro results ****')\n",
    "    f1_results = f1_score(df_results_full['y_true'],df_results_full['y_predicted'],average='micro')\n",
    "    \n",
    "    f1_random = f1_score(df_results_full['y_true'],\n",
    "             random_forecast['y_predicted'].loc[df_results_full['y_true'].index],average='micro')\n",
    "    df_week_0 = df_shifts[['week0']].dropna()\n",
    "    persistence_forecast = df_week_0.loc[df_results_full['y_true'].index]\n",
    "    f1_persistence = f1_score(df_results_full['y_true'],\n",
    "             persistence_forecast['week0'].loc[df_results_full['y_true'].index],average='micro')\n",
    "    \n",
    "    f1_climatology = f1_score(df_results_full['y_true'],\n",
    "             climatology_forecast['y_predicted'].loc[df_results_full['y_true'].index],average='micro')\n",
    "    print(f'F1 XGBoost:', f1_results)\n",
    "    print('F1 Random:',f1_random)\n",
    "    print('F1 Persistence:',f1_persistence)\n",
    "    print('F1 Climatology:',f1_climatology)\n",
    "    f1s_test2.append(f1_results)\n",
    "    f1s_random2.append(f1_random)\n",
    "    f1s_persistence2.append(f1_persistence)\n",
    "    f1s_climatology2.append(f1_climatology)\n",
    "    print('**** Frequencies ****')\n",
    "    print('True distribution:')\n",
    "    print(np.bincount(df_results_full['y_true'])/np.sum(np.bincount(df_results_full['y_true'])))\n",
    "    print('Forecasted distribution:')\n",
    "    print(np.bincount(df_results_full['y_predicted'])/np.sum(np.bincount(df_results_full['y_predicted'])))\n",
    "    end_time = datetime.now()\n",
    "    print('---------------> Running Time:',(end_time-start_time).seconds/60,' minutes.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch_wr]",
   "language": "python",
   "name": "conda-env-pytorch_wr-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
