{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b529883d-7b37-4494-8cac-e2e55aa64bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import copy\n",
    "from datetime import datetime, timedelta\n",
    "# from keras.utils import to_categorical\n",
    "# import visualkeras\n",
    "# import tensorflow as tf\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import brier_score_loss\n",
    "# import optuna\n",
    "# from optuna.samplers import TPESampler\n",
    "# import keras\n",
    "# from keras.callbacks import ModelCheckpoint\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import sys\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import glob \n",
    "\n",
    "\n",
    "from sklearn import datasets, ensemble\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.utils import class_weight\n",
    "import json\n",
    "\n",
    "import xgboost as xgb\n",
    "from bayes_opt import BayesianOptimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1154a5a6-5265-482b-a2da-24e636d29b64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Dec 23 15:52:31 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA A100-SXM4-80GB          On  |   00000000:01:00.0 Off |                    0 |\n",
      "| N/A   32C    P0             66W /  500W |       5MiB /  81920MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   1  NVIDIA A100-SXM4-80GB          On  |   00000000:41:00.0 Off |                    0 |\n",
      "| N/A   34C    P0             64W /  500W |       5MiB /  81920MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Run nvidia-smi to get GPU information\n",
    "os.system('nvidia-smi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ae086bb-2a73-4e5d-9c0f-9c48525ea935",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_id = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d0d57c2-0432-4f67-b8d0-f5995e49f636",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_val_test_periods(full_df):\n",
    "    dic_train_val = {}\n",
    "    dic_test = {}\n",
    "    \n",
    "    start_of_test_periods = np.arange(1981,2021,10)\n",
    "    end_of_test_periods = start_of_test_periods + 9\n",
    "    \n",
    "    for iperiod in range(len(start_of_test_periods)):\n",
    "        df_test_temp = full_df[str(start_of_test_periods[iperiod]):str(end_of_test_periods[iperiod])]\n",
    "        df_trainval_temp = full_df.drop(df_test_temp.index)\n",
    "        \n",
    "        dic_train_val[start_of_test_periods[iperiod]] = df_trainval_temp\n",
    "        dic_test[start_of_test_periods[iperiod]] = df_test_temp\n",
    "    return dic_train_val, dic_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "edfce0a8-3571-42b7-8f35-f6510a2a5424",
   "metadata": {},
   "outputs": [],
   "source": [
    "wr_original_series = pd.read_csv('/glade/work/jhayron/Data4Predictability/WR_Series_vSLtest.csv',\\\n",
    "                index_col=0,names=['week0','dist'],skiprows=1,parse_dates=True)\n",
    "# Rolling window for mode\n",
    "rolling_mode = (\n",
    "    wr_original_series.rolling('7d', center=True,min_periods=7)\n",
    "    .apply(lambda x: x.mode()[0] if not x.mode().empty else float('nan'))\n",
    ").shift(-3)\n",
    "\n",
    "# Rolling window for the count of the mode\n",
    "rolling_mode_count = (\n",
    "    wr_original_series.rolling('7d', center=True,min_periods=7)\n",
    "    .apply(lambda x: (x == x.mode()[0]).sum() if not x.mode().empty else 0)\n",
    ").shift(-3)\n",
    "\n",
    "# If duration of WR during week was less than 4, assing NO WR class\n",
    "rolling_mode.loc[rolling_mode_count['week0']<4,'week0'] = 4\n",
    "wr_series_mode = copy.deepcopy(rolling_mode)\n",
    "time_index = pd.to_datetime(wr_series_mode.index).dayofweek\n",
    "wr_series_mode = wr_series_mode.iloc[time_index.isin([0,3])].dropna()\n",
    "wr_series = copy.deepcopy(wr_series_mode)\n",
    "\n",
    "for wk in range(2,10):\n",
    "    series_temp = copy.deepcopy(wr_series[\"week0\"])\n",
    "    series_temp.index = series_temp.index - timedelta(weeks = wk-1)\n",
    "    series_temp.name = f'week{wk-1}'\n",
    "    if wk==2:\n",
    "        df_shifts = pd.concat([pd.DataFrame(wr_series[\"week0\"]),pd.DataFrame(series_temp)],axis=1)  \n",
    "    else:\n",
    "        df_shifts = pd.concat([df_shifts,pd.DataFrame(series_temp)],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3221fcb8-e398-4cf2-af8f-e89a12368e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_folders = np.sort(glob.glob('/glade/u/home/jhayron/WR_Predictability_v2/1_DataPreparation/PC_Data/PCs/*.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "23cce286-c722-486a-9df4-23179d9038df",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_vars = [list_folders[i].split('/')[-1][4:-4] for i in range(len(list_folders))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0242c757-a77d-4cfa-83a4-598cdff9b939",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 IC_SODA\n",
      "1 IT_SODA\n",
      "2 MLD_SODA\n",
      "3 OHC100_SODA\n",
      "4 OHC200_SODA\n",
      "5 OHC300_SODA\n",
      "6 OHC50_SODA\n",
      "7 OHC700_SODA\n",
      "8 OLR_ERA5\n",
      "9 SD_ERA5\n",
      "10 SSH_SODA\n",
      "11 SST_OISSTv2\n",
      "12 SST_SODA\n",
      "13 STL_1m_ERA5\n",
      "14 STL_28cm_ERA5\n",
      "15 STL_7cm_ERA5\n",
      "16 STL_full_ERA5\n",
      "17 SWVL_1m_ERA5\n",
      "18 SWVL_28cm_ERA5\n",
      "19 SWVL_7cm_ERA5\n",
      "20 SWVL_full_ERA5\n",
      "21 U10_ERA5\n",
      "22 U200_ERA5\n",
      "23 Z500_ERA5\n"
     ]
    }
   ],
   "source": [
    "for ivar,var in enumerate(list_vars):\n",
    "    print(ivar,var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a93c0b4e-451a-4592-8d70-4506eb60e37a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'SST_OISSTv2'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ivar = 11\n",
    "list_vars[ivar]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5192f326-aac3-4b9e-ad36-ce37a1b6dee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparams = {\n",
    "    \"n_estimators\": 100,       # Number of boosting rounds, balances speed and depth\n",
    "    \"max_depth\": 6,            # Controls tree depth; deeper trees model complexity better but risk overfitting\n",
    "    \"learning_rate\": 0.1,      # Step size shrinkage for reducing overfitting risk\n",
    "    \"subsample\": 0.8,          # Fraction of training samples used per tree, helps generalize\n",
    "    \"colsample_bytree\": 0.8,   # Fraction of features used per tree, improves robustness\n",
    "    \"gamma\": 0,                # Minimum loss reduction to make a split, prevents overfitting\n",
    "    \"min_child_weight\": 1,     # Minimum sum of instance weight needed in a leaf, controls complexity\n",
    "    \"reg_alpha\": 0,            # L1 regularization term for weights, often fine at 0 unless high sparsity is needed\n",
    "    \"reg_lambda\": 1            # L2 regularization term for weights, controls overfitting\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c1fe1950-3fe2-405c-a4af-c908be3b6346",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "week_out = 4\n",
    "print(week_out)\n",
    "week_out_str = f'week{week_out}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34619768-987b-4366-9509-2cc6508774ed",
   "metadata": {},
   "source": [
    "# Create benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c4fa7bd8-beaf-4d93-a2ed-a8cf8c0e9dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_week_0 = df_shifts[['week0']].dropna()\n",
    "\n",
    "def generate_random_forecast(df_week_0, seed_value=42):\n",
    "    # Set the random seed for reproducibility\n",
    "    np.random.seed(seed_value)\n",
    "    \n",
    "    # Step 1: Get unique classes and their frequencies\n",
    "    values = df_week_0['week0'].value_counts()\n",
    "    \n",
    "    # Step 2: Calculate the probabilities for each class\n",
    "    classes = values.index  # Unique classes\n",
    "    probabilities = values / values.sum()  # Normalize to get probability distribution\n",
    "    \n",
    "    # Step 3: Generate a random forecast based on the probabilities\n",
    "    random_forecast = np.random.choice(classes, size=len(df_week_0), p=probabilities)\n",
    "    \n",
    "    # Step 4: Return the random forecast as a DataFrame or Series\n",
    "    forecast_df = pd.DataFrame(random_forecast, index=df_week_0.index, columns=['y_predicted'])\n",
    "    \n",
    "    return forecast_df\n",
    "\n",
    "def generate_random_forecast_probabilities(df_week_0, seed_value=42):\n",
    "    # Set the random seed for reproducibility\n",
    "    np.random.seed(seed_value)\n",
    "    \n",
    "    # Step 1: Get unique classes and their frequencies\n",
    "    values = df_week_0['week0'].value_counts()\n",
    "    \n",
    "    # Step 2: Calculate the probabilities for each class\n",
    "    classes = values.index  # Unique classes\n",
    "    probabilities = values / values.sum()  # Normalize to get probability distribution\n",
    "    \n",
    "    # Step 3: Create a probability forecast for each sample\n",
    "    # Create a 2D array where each row is the same probability distribution\n",
    "    prob_matrix = np.tile(probabilities.values, (len(df_week_0), 1))\n",
    "    \n",
    "    # Step 4: Return the probability matrix as a DataFrame\n",
    "    forecast_df = pd.DataFrame(prob_matrix, index=df_week_0.index, columns=classes)[np.arange(5)]\n",
    "    \n",
    "    return forecast_df\n",
    "\n",
    "def generate_random_forecast_with_monthly_probabilities(df_week_0, seed_value=42):\n",
    "    # Set the random seed for reproducibility\n",
    "    np.random.seed(seed_value)\n",
    "    \n",
    "    # Extract the month from the index (assuming the index is a datetime index)\n",
    "    df_week_0['month'] = df_week_0.index.month\n",
    "    \n",
    "    # Prepare an empty list to store the random forecast\n",
    "    forecasts = []\n",
    "    \n",
    "    # Loop through each month\n",
    "    for month in range(1, 13):  # Loop through months 1 to 12\n",
    "        # Filter data for the current month\n",
    "        month_data = df_week_0[df_week_0['month'] == month]\n",
    "        \n",
    "        # Step 1: Get unique classes and their frequencies for the current month\n",
    "        values = month_data['week0'].value_counts()\n",
    "        \n",
    "        # Step 2: Calculate the probabilities for each class in the current month\n",
    "        classes = values.index  # Unique classes\n",
    "        probabilities = values / values.sum()  # Normalize to get probability distribution\n",
    "        \n",
    "        # Step 3: Generate random forecasts for the current month based on the probabilities\n",
    "        month_forecast = np.random.choice(classes, size=len(month_data), p=probabilities)\n",
    "        \n",
    "        # Store the forecast for the current month\n",
    "        forecasts.append(pd.Series(month_forecast, index=month_data.index))\n",
    "    \n",
    "    # Combine all monthly forecasts into one DataFrame\n",
    "    forecast_df = pd.concat(forecasts)\n",
    "    forecast_df = forecast_df.sort_index()  # Sort the index to preserve the original order\n",
    "    forecast_df = pd.DataFrame(forecast_df,columns=['y_predicted'])\n",
    "    return forecast_df\n",
    "    \n",
    "def generate_probability_forecast_with_monthly_probabilities(df_week_0, seed_value=42):\n",
    "    # Set the random seed for reproducibility\n",
    "    np.random.seed(seed_value)\n",
    "    \n",
    "    # Extract the month from the index (assuming the index is a datetime index)\n",
    "    df_week_0['month'] = df_week_0.index.month\n",
    "    \n",
    "    # Prepare an empty DataFrame to store the probability forecasts\n",
    "    all_probabilities = pd.DataFrame(index=df_week_0.index)\n",
    "    \n",
    "    # Loop through each month\n",
    "    for month in range(1, 13):  # Loop through months 1 to 12\n",
    "        # Filter data for the current month\n",
    "        month_data = df_week_0[df_week_0['month'] == month]\n",
    "        \n",
    "        if month_data.empty:\n",
    "            continue  # Skip if there's no data for the month\n",
    "        \n",
    "        # Step 1: Get unique classes and their frequencies for the current month\n",
    "        values = month_data['week0'].value_counts()\n",
    "        \n",
    "        # Step 2: Calculate the probabilities for each class in the current month\n",
    "        classes = values.index  # Unique classes\n",
    "        probabilities = values / values.sum()  # Normalize to get probability distribution\n",
    "        \n",
    "        # Step 3: Create a probability matrix for the current month\n",
    "        prob_matrix = np.tile(probabilities.values, (len(month_data), 1))\n",
    "        \n",
    "        # Create a DataFrame for this month's probabilities with appropriate columns\n",
    "        month_prob_df = pd.DataFrame(prob_matrix, index=month_data.index, columns=classes)\n",
    "        \n",
    "        # Append this month's DataFrame to the overall probability DataFrame\n",
    "        all_probabilities = pd.concat([all_probabilities, month_prob_df])\n",
    "    \n",
    "    # Sort the index to match the original order\n",
    "    all_probabilities = all_probabilities.sort_index()\n",
    "    \n",
    "    # Fill missing columns with zeros for months that do not include certain classes\n",
    "    all_classes = df_week_0['week0'].unique()\n",
    "    all_probabilities = all_probabilities.reindex(columns=all_classes, fill_value=0).dropna()[np.arange(5)]\n",
    "    \n",
    "    return all_probabilities\n",
    "\n",
    "random_forecast = generate_random_forecast(df_week_0, seed_value=42)\n",
    "climatology_forecast = generate_random_forecast_with_monthly_probabilities(df_week_0, seed_value=42)\n",
    "random_forecast_probs = generate_random_forecast_probabilities(df_week_0)\n",
    "climatology_forecast_probs = generate_probability_forecast_with_monthly_probabilities(df_week_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00b174d-5dd9-4e43-bcf8-f41d20fb6856",
   "metadata": {},
   "source": [
    "# Changing number of PCs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3e25eb69-86d7-4005-95f4-bd6b069c5dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_pcs = [5,10,20,50,None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f6331cdc-8740-4cfd-b5d0-72f08323bd28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/glade/work/jhayron/conda-envs/xgb_wr/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [15:53:18] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1730232887822/work/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
      "Potential solutions:\n",
      "- Use a data structure that matches the device ordinal in the booster.\n",
      "- Set the device for booster before call to inplace_predict.\n",
      "\n",
      "This warning will only be shown once.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 XGBoost - 5 PCs: 0.19164004353941422\n",
      "F1 XGBoost - 10 PCs: 0.1811487173948187\n",
      "F1 XGBoost - 20 PCs: 0.20820142790172222\n",
      "F1 XGBoost - 50 PCs: 0.18894829618388012\n",
      "F1 XGBoost - None PCs: 0.1854132423105931\n"
     ]
    }
   ],
   "source": [
    "for npc in n_pcs:\n",
    "    combined_df = pd.read_csv(f'/glade/u/home/jhayron/WR_Predictability_v2/1_DataPreparation/PC_Data/PCs/PCs_{list_vars[ivar]}.csv',\n",
    "                              index_col=0,\n",
    "                              parse_dates=True)\n",
    "    combined_df = combined_df.iloc[:,:npc]\n",
    "\n",
    "    \n",
    "    # combined_df = copy.deepcopy(anoms_flattened)\n",
    "    # combined_df['day_sin'] = np.sin(2 * np.pi * combined_df.index.day_of_year / 365)\n",
    "    # combined_df['day_cos'] = np.cos(2 * np.pi * combined_df.index.day_of_year / 365)\n",
    "    \n",
    "    fully_combined_df = pd.concat([combined_df,df_shifts[week_out_str]],axis=1)\n",
    "    fully_combined_df = fully_combined_df.dropna()\n",
    "    dic_trainval, dic_test = get_train_val_test_periods(fully_combined_df)\n",
    "    start_of_test_periods = np.arange(1981,2021,10)\n",
    "    list_results = []\n",
    "    list_results_probs = []\n",
    "    \n",
    "    for iperiod in range(len(start_of_test_periods)):\n",
    "        X_trainval = dic_trainval[start_of_test_periods[iperiod]].iloc[:,:-1].values\n",
    "        y_trainval = dic_trainval[start_of_test_periods[iperiod]].iloc[:,-1]\n",
    "    \n",
    "        X_test = dic_test[start_of_test_periods[iperiod]].iloc[:,:-1].values\n",
    "        y_test = dic_test[start_of_test_periods[iperiod]].iloc[:,-1]\n",
    "        \n",
    "        cw = class_weight.compute_sample_weight(\n",
    "            class_weight='balanced',\n",
    "            y=y_trainval\n",
    "        )\n",
    "        model = xgb.XGBClassifier(n_estimators=100,\n",
    "                                max_depth=hyperparams['max_depth'],\n",
    "                                learning_rate=hyperparams['learning_rate'],\n",
    "                                subsample=hyperparams['subsample'],\n",
    "                                colsample_bytree=hyperparams['colsample_bytree'],\n",
    "                                # colsample_bylevel=best_params['colsample_bylevel'],\n",
    "                                gamma=hyperparams['gamma'],\n",
    "                                reg_alpha=hyperparams['reg_alpha'],\n",
    "                                reg_lambda=hyperparams['reg_lambda'],\n",
    "                                num_class=5,\n",
    "                                objective = \"multi:softprob\",\n",
    "                                tree_method='hist',\n",
    "                                device = f'cuda:{gpu_id}')\n",
    "        model.fit(X_trainval, y_trainval, sample_weight=cw)\n",
    "        y_predicted = model.predict(X_test)\n",
    "        y_predicted_probs = model.predict_proba(X_test)\n",
    "        y_predicted_probs = pd.DataFrame(y_predicted_probs,index=y_test.index)\n",
    "        df_results_temp = pd.DataFrame(np.array([y_test.values,y_predicted]).T,\n",
    "                                       index=y_test.index,\n",
    "                                       columns=['y_true','y_predicted'])\n",
    "        list_results.append(df_results_temp)\n",
    "        list_results_probs.append(y_predicted_probs)\n",
    "        \n",
    "    df_results_full = pd.concat(list_results,axis=0)\n",
    "    df_results_full_probs = pd.concat(list_results_probs,axis=0)\n",
    "\n",
    "    f1_results = f1_score(df_results_full['y_true'],df_results_full['y_predicted'],average='macro')\n",
    "    print(f'F1 XGBoost - {npc} PCs:', f1_results)\n",
    "\n",
    "    # y_true_onehot = OneHotEncoder(sparse_output=False).\\\n",
    "    #     fit_transform(df_results_full['y_true'].values.reshape(-1, 1))\n",
    "    # b_score_results = np.mean(np.sum((df_results_full_probs.values - y_true_onehot) ** 2, axis=1))\n",
    "    # print(f'BScore XGBoost - {npc} PCs:', b_score_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ca1f700e-e0c4-4f28-8110-d0435f6cb42e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Random: 0.20321186876521127\n",
      "F1 Persistence: 0.21727016161892304\n",
      "F1 Climatology: 0.2179900312067644\n"
     ]
    }
   ],
   "source": [
    "f1_random = f1_score(df_results_full['y_true'],\n",
    "         random_forecast['y_predicted'].loc[df_results_full['y_true'].index],average='macro')\n",
    "\n",
    "persistence_forecast = df_week_0.loc[df_results_full['y_true'].index]\n",
    "f1_persistence = f1_score(df_results_full['y_true'],\n",
    "         persistence_forecast['week0'].loc[df_results_full['y_true'].index],average='macro')\n",
    "\n",
    "f1_climatology = f1_score(df_results_full['y_true'],\n",
    "         climatology_forecast['y_predicted'].loc[df_results_full['y_true'].index],average='macro')\n",
    "\n",
    "print('F1 Random:',f1_random)\n",
    "print('F1 Persistence:',f1_persistence)\n",
    "print('F1 Climatology:',f1_climatology)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f80e53-d7f2-4836-a5b9-cf86c038892a",
   "metadata": {},
   "source": [
    "# PCs vs. all pixels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9176e274-bf97-44dd-9f42-d7f423928860",
   "metadata": {},
   "source": [
    "## Load all pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8b4100af-008a-4ee4-ad44-1f3754ecba89",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_weekly_anoms = '/glade/derecho/scratch/jhayron/Data4Predictability/WeeklyAnoms_DetrendedStd_v3_2dg/'\n",
    "path_nc_anoms = f'{path_weekly_anoms}{list_vars[ivar]}.nc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1fb1a064-0a93-4dfa-9038-efef7a6aed49",
   "metadata": {},
   "outputs": [],
   "source": [
    "anoms = xr.open_dataset(path_nc_anoms)\n",
    "var_name_nc = list(anoms.data_vars.keys())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c1f2d8a1-175c-46b9-b545-0ffd00879d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a boxcar filter function\n",
    "def boxcar_filter(data, size):\n",
    "    kernel = np.ones((size, size)) / (size * size)\n",
    "    from scipy.signal import convolve2d\n",
    "    return convolve2d(data, kernel, mode=\"same\", boundary=\"fill\", fillvalue=np.nan)\n",
    "\n",
    "# Apply boxcar filter\n",
    "smoothed_anoms = xr.apply_ufunc(\n",
    "    boxcar_filter,\n",
    "    anoms,\n",
    "    kwargs={\"size\": 3},  # Adjust window size (e.g., 5x5 grid cells)\n",
    "    input_core_dims=[[\"lat\", \"lon\"]],\n",
    "    output_core_dims=[[\"lat\", \"lon\"]],\n",
    "    vectorize=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "decbe8c1-228a-4dd7-a935-3b727e37b89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "anoms_flattened = smoothed_anoms[var_name_nc].stack(flat_spatial=('lat', 'lon'))\n",
    "anoms_flattened = pd.DataFrame(anoms_flattened,index = anoms_flattened.time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "dfcb79f4-e752-4e80-92e6-3b6fe10aa8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "anoms_flattened = anoms_flattened.dropna(axis=1, how='any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "01f2494d-fcc8-4772-a8f7-9901eb1af6c1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "combined_df = copy.deepcopy(anoms_flattened)\n",
    "# combined_df['day_sin'] = np.sin(2 * np.pi * combined_df.index.day_of_year / 365)\n",
    "# combined_df['day_cos'] = np.cos(2 * np.pi * combined_df.index.day_of_year / 365)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "00108790-c024-42bc-a186-e6946a0586fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "fully_combined_df = pd.concat([combined_df,df_shifts[week_out_str]],axis=1)\n",
    "fully_combined_df = fully_combined_df.dropna()\n",
    "dic_trainval, dic_test = get_train_val_test_periods(fully_combined_df)\n",
    "start_of_test_periods = np.arange(1981,2021,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "08ca34e9-28be-4959-b151-d23b7049ab7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "list_results = []\n",
    "list_results_probs = []\n",
    "\n",
    "for iperiod in range(len(start_of_test_periods)):\n",
    "    print(iperiod)\n",
    "    X_trainval = dic_trainval[start_of_test_periods[iperiod]].iloc[:,:-1].values\n",
    "    y_trainval = dic_trainval[start_of_test_periods[iperiod]].iloc[:,-1]\n",
    "\n",
    "    X_test = dic_test[start_of_test_periods[iperiod]].iloc[:,:-1].values\n",
    "    y_test = dic_test[start_of_test_periods[iperiod]].iloc[:,-1]\n",
    "    \n",
    "    cw = class_weight.compute_sample_weight(\n",
    "        class_weight='balanced',\n",
    "        y=y_trainval\n",
    "    )\n",
    "\n",
    "    model = xgb.XGBClassifier(n_estimators=100,\n",
    "                            max_depth=hyperparams['max_depth'],\n",
    "                            learning_rate=hyperparams['learning_rate'],\n",
    "                            subsample=hyperparams['subsample'],\n",
    "                            colsample_bytree=hyperparams['colsample_bytree'],\n",
    "                            # colsample_bylevel=best_params['colsample_bylevel'],\n",
    "                            gamma=hyperparams['gamma'],\n",
    "                            reg_alpha=hyperparams['reg_alpha'],\n",
    "                            reg_lambda=hyperparams['reg_lambda'],\n",
    "                            num_class=5,\n",
    "                            objective = \"multi:softprob\",\n",
    "                            tree_method='hist',\n",
    "                            device = f'cuda:{gpu_id}')\n",
    "    model.fit(X_trainval, y_trainval, sample_weight=cw)\n",
    "    y_predicted = model.predict(X_test)\n",
    "    y_predicted_probs = model.predict_proba(X_test)\n",
    "    y_predicted_probs = pd.DataFrame(y_predicted_probs,index=y_test.index)\n",
    "    df_results_temp = pd.DataFrame(np.array([y_test.values,y_predicted]).T,\n",
    "                                   index=y_test.index,\n",
    "                                   columns=['y_true','y_predicted'])\n",
    "    list_results.append(df_results_temp)\n",
    "    list_results_probs.append(y_predicted_probs)\n",
    "    \n",
    "df_results_full = pd.concat(list_results,axis=0)\n",
    "df_results_full_probs = pd.concat(list_results_probs,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1f3c7fc0-f847-4da2-9a24-5a71dc86f2f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 XGBoost: 0.2721055202735711\n",
      "F1 Random: 0.21812408402540304\n",
      "F1 Persistence: 0.23326819736199317\n",
      "F1 Climatology: 0.23277967757694187\n"
     ]
    }
   ],
   "source": [
    "f1_results = f1_score(df_results_full['y_true'],df_results_full['y_predicted'],average='micro')\n",
    "\n",
    "f1_random = f1_score(df_results_full['y_true'],\n",
    "         random_forecast['y_predicted'].loc[df_results_full['y_true'].index],average='micro')\n",
    "\n",
    "persistence_forecast = df_week_0.loc[df_results_full['y_true'].index]\n",
    "f1_persistence = f1_score(df_results_full['y_true'],\n",
    "         persistence_forecast['week0'].loc[df_results_full['y_true'].index],average='micro')\n",
    "\n",
    "f1_climatology = f1_score(df_results_full['y_true'],\n",
    "         climatology_forecast['y_predicted'].loc[df_results_full['y_true'].index],average='micro')\n",
    "print(f'F1 XGBoost:', f1_results)\n",
    "print('F1 Random:',f1_random)\n",
    "print('F1 Persistence:',f1_persistence)\n",
    "print('F1 Climatology:',f1_climatology)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "aee690c2-f4c6-4e44-9984-c08ff357a6e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(16.649214659685867)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "100*(f1_results - f1_persistence)/f1_persistence"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:xgb_wr]",
   "language": "python",
   "name": "conda-env-xgb_wr-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
