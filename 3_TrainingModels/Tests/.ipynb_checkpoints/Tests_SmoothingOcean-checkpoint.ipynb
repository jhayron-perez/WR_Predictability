{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b529883d-7b37-4494-8cac-e2e55aa64bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import copy\n",
    "from datetime import datetime, timedelta\n",
    "# from keras.utils import to_categorical\n",
    "# import visualkeras\n",
    "# import tensorflow as tf\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "# import optuna\n",
    "# from optuna.samplers import TPESampler\n",
    "# import keras\n",
    "# from keras.callbacks import ModelCheckpoint\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import sys\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import glob \n",
    "\n",
    "\n",
    "from sklearn import datasets, ensemble\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.utils import class_weight\n",
    "import json\n",
    "\n",
    "import xgboost as xgb\n",
    "from bayes_opt import BayesianOptimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1154a5a6-5265-482b-a2da-24e636d29b64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Nov  7 09:44:50 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  Tesla V100-SXM2-32GB           On  |   00000000:62:00.0 Off |                    0 |\n",
      "| N/A   33C    P0             41W /  300W |       5MiB /  32768MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Run nvidia-smi to get GPU information\n",
    "os.system('nvidia-smi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ae086bb-2a73-4e5d-9c0f-9c48525ea935",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_id = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d0d57c2-0432-4f67-b8d0-f5995e49f636",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_val_test_periods(full_df):\n",
    "    dic_train_val = {}\n",
    "    dic_test = {}\n",
    "    \n",
    "    start_of_test_periods = np.arange(1981,2021,10)\n",
    "    end_of_test_periods = start_of_test_periods + 9\n",
    "    \n",
    "    for iperiod in range(len(start_of_test_periods)):\n",
    "        df_test_temp = full_df[str(start_of_test_periods[iperiod]):str(end_of_test_periods[iperiod])]\n",
    "        df_trainval_temp = full_df.drop(df_test_temp.index)\n",
    "        \n",
    "        dic_train_val[start_of_test_periods[iperiod]] = df_trainval_temp\n",
    "        dic_test[start_of_test_periods[iperiod]] = df_test_temp\n",
    "    return dic_train_val, dic_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "edfce0a8-3571-42b7-8f35-f6510a2a5424",
   "metadata": {},
   "outputs": [],
   "source": [
    "wr_series = pd.read_csv('/glade/work/jhayron/Data4Predictability/WR_Series_20241023.csv',\\\n",
    "                index_col=0,names=['week0'],skiprows=1,parse_dates=True)\n",
    "for wk in range(2,10):\n",
    "    series_temp = copy.deepcopy(wr_series[\"week0\"])\n",
    "    series_temp.index = series_temp.index - timedelta(weeks = wk-1)\n",
    "    series_temp.name = f'week{wk-1}'\n",
    "    if wk==2:\n",
    "        df_shifts = pd.concat([pd.DataFrame(wr_series[\"week0\"]),pd.DataFrame(series_temp)],axis=1)  \n",
    "    else:\n",
    "        df_shifts = pd.concat([df_shifts,pd.DataFrame(series_temp)],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3221fcb8-e398-4cf2-af8f-e89a12368e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_folders = np.sort(glob.glob('/glade/u/home/jhayron/WR_Predictability/6_PCA_xgboost/PC_Data/PCs/*.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "23cce286-c722-486a-9df4-23179d9038df",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_vars = [list_folders[i].split('/')[-1][4:-4] for i in range(len(list_folders))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0242c757-a77d-4cfa-83a4-598cdff9b939",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 IC_SODA\n",
      "1 IC_SODA_wTrend\n",
      "2 IT_SODA\n",
      "3 IT_SODA_wTrend\n",
      "4 MLD_SODA\n",
      "5 MLD_SODA_wTrend\n",
      "6 OHC100_SODA\n",
      "7 OHC100_SODA_wTrend\n",
      "8 OHC200_SODA\n",
      "9 OHC200_SODA_wTrend\n",
      "10 OHC300_SODA\n",
      "11 OHC300_SODA_wTrend\n",
      "12 OHC50_SODA\n",
      "13 OHC50_SODA_wTrend\n",
      "14 OHC700_SODA\n",
      "15 OHC700_SODA_wTrend\n",
      "16 OLR_ERA5\n",
      "17 OLR_ERA5_wTrend\n",
      "18 SD_ERA5\n",
      "19 SD_ERA5_wTrend\n",
      "20 SSH_SODA\n",
      "21 SSH_SODA_wTrend\n",
      "22 SST_OISSTv2\n",
      "23 SST_OISSTv2_wTrend\n",
      "24 SST_SODA\n",
      "25 SST_SODA_wTrend\n",
      "26 STL_1m_ERA5\n",
      "27 STL_1m_ERA5_wTrend\n",
      "28 STL_28cm_ERA5\n",
      "29 STL_28cm_ERA5_wTrend\n",
      "30 STL_7cm_ERA5\n",
      "31 STL_7cm_ERA5_wTrend\n",
      "32 STL_full_ERA5\n",
      "33 STL_full_ERA5_wTrend\n",
      "34 SWVL_1m_ERA5\n",
      "35 SWVL_1m_ERA5_wTrend\n",
      "36 SWVL_28cm_ERA5\n",
      "37 SWVL_28cm_ERA5_wTrend\n",
      "38 SWVL_7cm_ERA5\n",
      "39 SWVL_7cm_ERA5_wTrend\n",
      "40 SWVL_full_ERA5\n",
      "41 SWVL_full_ERA5_wTrend\n",
      "42 U10_ERA5\n",
      "43 U10_ERA5_wTrend\n",
      "44 U200_ERA5\n",
      "45 U200_ERA5_wTrend\n",
      "46 Z500_ERA5\n",
      "47 Z500_ERA5_Region\n",
      "48 Z500_ERA5_wTrend\n"
     ]
    }
   ],
   "source": [
    "for ivar,var in enumerate(list_vars):\n",
    "    print(ivar,var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a93c0b4e-451a-4592-8d70-4506eb60e37a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'OHC300_SODA'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ivar = 10\n",
    "list_vars[ivar]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5192f326-aac3-4b9e-ad36-ce37a1b6dee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparams = {\n",
    "    \"n_estimators\": 100,       # Number of boosting rounds, balances speed and depth\n",
    "    \"max_depth\": 6,            # Controls tree depth; deeper trees model complexity better but risk overfitting\n",
    "    \"learning_rate\": 0.1,      # Step size shrinkage for reducing overfitting risk\n",
    "    \"subsample\": 0.8,          # Fraction of training samples used per tree, helps generalize\n",
    "    \"colsample_bytree\": 0.8,   # Fraction of features used per tree, improves robustness\n",
    "    \"gamma\": 0,                # Minimum loss reduction to make a split, prevents overfitting\n",
    "    \"min_child_weight\": 1,     # Minimum sum of instance weight needed in a leaf, controls complexity\n",
    "    \"reg_alpha\": 0,            # L1 regularization term for weights, often fine at 0 unless high sparsity is needed\n",
    "    \"reg_lambda\": 1            # L2 regularization term for weights, controls overfitting\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c1fe1950-3fe2-405c-a4af-c908be3b6346",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "week_out = 5\n",
    "print(week_out)\n",
    "week_out_str = f'week{week_out}'\n",
    "combined_df = pd.read_csv(f'../PC_Data/PCs/PCs_{list_vars[ivar]}.csv',\n",
    "                          index_col=0,\n",
    "                          parse_dates=True)\n",
    "fully_combined_df = pd.concat([combined_df,df_shifts[week_out_str]],axis=1)\n",
    "fully_combined_df = fully_combined_df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34619768-987b-4366-9509-2cc6508774ed",
   "metadata": {},
   "source": [
    "# Create benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c4fa7bd8-beaf-4d93-a2ed-a8cf8c0e9dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_week_0 = df_shifts[['week0']].dropna()\n",
    "\n",
    "def generate_random_forecast(df_week_0, seed_value=42):\n",
    "    # Set the random seed for reproducibility\n",
    "    np.random.seed(seed_value)\n",
    "    \n",
    "    # Step 1: Get unique classes and their frequencies\n",
    "    values = df_week_0['week0'].value_counts()\n",
    "    \n",
    "    # Step 2: Calculate the probabilities for each class\n",
    "    classes = values.index  # Unique classes\n",
    "    probabilities = values / values.sum()  # Normalize to get probability distribution\n",
    "    \n",
    "    # Step 3: Generate a random forecast based on the probabilities\n",
    "    random_forecast = np.random.choice(classes, size=len(df_week_0), p=probabilities)\n",
    "    \n",
    "    # Step 4: Return the random forecast as a DataFrame or Series\n",
    "    forecast_df = pd.DataFrame(random_forecast, index=df_week_0.index, columns=['y_predicted'])\n",
    "    \n",
    "    return forecast_df\n",
    "\n",
    "def generate_random_forecast_with_monthly_probabilities(df_week_0, seed_value=42):\n",
    "    # Set the random seed for reproducibility\n",
    "    np.random.seed(seed_value)\n",
    "    \n",
    "    # Extract the month from the index (assuming the index is a datetime index)\n",
    "    df_week_0['month'] = df_week_0.index.month\n",
    "    \n",
    "    # Prepare an empty list to store the random forecast\n",
    "    forecasts = []\n",
    "    \n",
    "    # Loop through each month\n",
    "    for month in range(1, 13):  # Loop through months 1 to 12\n",
    "        # Filter data for the current month\n",
    "        month_data = df_week_0[df_week_0['month'] == month]\n",
    "        \n",
    "        # Step 1: Get unique classes and their frequencies for the current month\n",
    "        values = month_data['week0'].value_counts()\n",
    "        \n",
    "        # Step 2: Calculate the probabilities for each class in the current month\n",
    "        classes = values.index  # Unique classes\n",
    "        probabilities = values / values.sum()  # Normalize to get probability distribution\n",
    "        \n",
    "        # Step 3: Generate random forecasts for the current month based on the probabilities\n",
    "        month_forecast = np.random.choice(classes, size=len(month_data), p=probabilities)\n",
    "        \n",
    "        # Store the forecast for the current month\n",
    "        forecasts.append(pd.Series(month_forecast, index=month_data.index))\n",
    "    \n",
    "    # Combine all monthly forecasts into one DataFrame\n",
    "    forecast_df = pd.concat(forecasts)\n",
    "    forecast_df = forecast_df.sort_index()  # Sort the index to preserve the original order\n",
    "    forecast_df = pd.DataFrame(forecast_df,columns=['y_predicted'])\n",
    "    return forecast_df\n",
    "\n",
    "# Example usage\n",
    "random_forecast = generate_random_forecast(df_week_0, seed_value=42)\n",
    "climatology_forecast = generate_random_forecast_with_monthly_probabilities(df_week_0, seed_value=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5b6dccfb-1642-4074-9857-e113c1d6284b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 XGBoost - 10 PCs: 0.22171401059220028\n",
      "F1 Random: 0.202696196437169\n",
      "F1 Persistence: 0.21762156957149736\n",
      "F1 Climatology: 0.205825710158883\n"
     ]
    }
   ],
   "source": [
    "# Without day of year\n",
    "npc = 10\n",
    "combined_df = pd.read_csv(f'../PC_Data/PCs/PCs_{list_vars[ivar]}.csv',\n",
    "                          index_col=0,\n",
    "                          parse_dates=True)\n",
    "combined_df = combined_df.iloc[:,:npc]\n",
    "# # Normalize day of the year using sine and cosine transformations\n",
    "# combined_df['day_sin'] = np.sin(2 * np.pi * combined_df.index.day_of_year / 365)\n",
    "# combined_df['day_cos'] = np.cos(2 * np.pi * combined_df.index.day_of_year / 365)\n",
    "\n",
    "fully_combined_df = pd.concat([combined_df,df_shifts[week_out_str]],axis=1)\n",
    "fully_combined_df = fully_combined_df.dropna()\n",
    "dic_trainval, dic_test = get_train_val_test_periods(fully_combined_df)\n",
    "start_of_test_periods = np.arange(1981,2021,10)\n",
    "list_results = []\n",
    "\n",
    "for iperiod in range(len(start_of_test_periods)):\n",
    "    X_trainval = dic_trainval[start_of_test_periods[iperiod]].iloc[:,:-1].values\n",
    "    y_trainval = dic_trainval[start_of_test_periods[iperiod]].iloc[:,-1]\n",
    "\n",
    "    X_test = dic_test[start_of_test_periods[iperiod]].iloc[:,:-1].values\n",
    "    y_test = dic_test[start_of_test_periods[iperiod]].iloc[:,-1]\n",
    "    \n",
    "    cw = class_weight.compute_sample_weight(\n",
    "        class_weight='balanced',\n",
    "        y=y_trainval\n",
    "    )\n",
    "    model = xgb.XGBClassifier(n_estimators=100,\n",
    "                            max_depth=hyperparams['max_depth'],\n",
    "                            learning_rate=hyperparams['learning_rate'],\n",
    "                            subsample=hyperparams['subsample'],\n",
    "                            colsample_bytree=hyperparams['colsample_bytree'],\n",
    "                            # colsample_bylevel=best_params['colsample_bylevel'],\n",
    "                            gamma=hyperparams['gamma'],\n",
    "                            reg_alpha=hyperparams['reg_alpha'],\n",
    "                            reg_lambda=hyperparams['reg_lambda'],\n",
    "                            num_class=5,\n",
    "                            objective = \"multi:softprob\",\n",
    "                            tree_method='hist',\n",
    "                            device = f'cuda:{gpu_id}')\n",
    "    model.fit(X_trainval, y_trainval, sample_weight=cw)\n",
    "    y_predicted = model.predict(X_test)\n",
    "    df_results_temp = pd.DataFrame(np.array([y_test.values,y_predicted]).T,\n",
    "                                   index=y_test.index,\n",
    "                                   columns=['y_true','y_predicted'])\n",
    "    list_results.append(df_results_temp)\n",
    "    \n",
    "df_results_full = pd.concat(list_results,axis=0)\n",
    "\n",
    "f1_results = f1_score(df_results_full['y_true'],df_results_full['y_predicted'],average='micro')\n",
    "\n",
    "f1_random = f1_score(df_results_full['y_true'],\n",
    "         random_forecast['y_predicted'].loc[df_results_full['y_true'].index],average='micro')\n",
    "\n",
    "persistence_forecast = df_week_0.loc[df_results_full['y_true'].index]\n",
    "f1_persistence = f1_score(df_results_full['y_true'],\n",
    "         persistence_forecast['week0'].loc[df_results_full['y_true'].index],average='micro')\n",
    "\n",
    "f1_climatology = f1_score(df_results_full['y_true'],\n",
    "         climatology_forecast['y_predicted'].loc[df_results_full['y_true'].index],average='micro')\n",
    "print(f'F1 XGBoost - {npc} PCs:', f1_results)\n",
    "print('F1 Random:',f1_random)\n",
    "print('F1 Persistence:',f1_persistence)\n",
    "print('F1 Climatology:',f1_climatology)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "591deaaf-0644-4982-85e7-e3e0a05231d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# With day of year\n",
    "npc = 10\n",
    "combined_df = pd.read_csv(f'../PC_Data/PCs/PCs_{list_vars[ivar]}.csv',\n",
    "                          index_col=0,\n",
    "                          parse_dates=True)\n",
    "combined_df = combined_df.iloc[:,:npc]\n",
    "# Normalize day of the year using sine and cosine transformations\n",
    "combined_df['day_sin'] = np.sin(2 * np.pi * combined_df.index.day_of_year / 365)\n",
    "combined_df['day_cos'] = np.cos(2 * np.pi * combined_df.index.day_of_year / 365)\n",
    "\n",
    "fully_combined_df = pd.concat([combined_df,df_shifts[week_out_str]],axis=1)\n",
    "fully_combined_df = fully_combined_df.dropna()\n",
    "dic_trainval, dic_test = get_train_val_test_periods(fully_combined_df)\n",
    "start_of_test_periods = np.arange(1981,2021,10)\n",
    "list_results = []\n",
    "\n",
    "for iperiod in range(len(start_of_test_periods)):\n",
    "    X_trainval = dic_trainval[start_of_test_periods[iperiod]].iloc[:,:-1].values\n",
    "    y_trainval = dic_trainval[start_of_test_periods[iperiod]].iloc[:,-1]\n",
    "\n",
    "    X_test = dic_test[start_of_test_periods[iperiod]].iloc[:,:-1].values\n",
    "    y_test = dic_test[start_of_test_periods[iperiod]].iloc[:,-1]\n",
    "    \n",
    "    cw = class_weight.compute_sample_weight(\n",
    "        class_weight='balanced',\n",
    "        y=y_trainval\n",
    "    )\n",
    "    model = xgb.XGBClassifier(n_estimators=100,\n",
    "                            max_depth=hyperparams['max_depth'],\n",
    "                            learning_rate=hyperparams['learning_rate'],\n",
    "                            subsample=hyperparams['subsample'],\n",
    "                            colsample_bytree=hyperparams['colsample_bytree'],\n",
    "                            # colsample_bylevel=best_params['colsample_bylevel'],\n",
    "                            gamma=hyperparams['gamma'],\n",
    "                            reg_alpha=hyperparams['reg_alpha'],\n",
    "                            reg_lambda=hyperparams['reg_lambda'],\n",
    "                            num_class=5,\n",
    "                            objective = \"multi:softprob\",\n",
    "                            tree_method='hist',\n",
    "                            device = f'cuda:{gpu_id}')\n",
    "    model.fit(X_trainval, y_trainval, sample_weight=cw)\n",
    "    y_predicted = model.predict(X_test)\n",
    "    df_results_temp = pd.DataFrame(np.array([y_test.values,y_predicted]).T,\n",
    "                                   index=y_test.index,\n",
    "                                   columns=['y_true','y_predicted'])\n",
    "    list_results.append(df_results_temp)\n",
    "    \n",
    "df_results_full = pd.concat(list_results,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "06e5fc66-26cb-4565-b71b-9a1c5c950bf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 XGBoost - 10 PCs: 0.24795377948964853\n",
      "F1 Random: 0.202696196437169\n",
      "F1 Persistence: 0.21762156957149736\n",
      "F1 Climatology: 0.205825710158883\n"
     ]
    }
   ],
   "source": [
    "f1_results = f1_score(df_results_full['y_true'],df_results_full['y_predicted'],average='micro')\n",
    "\n",
    "f1_random = f1_score(df_results_full['y_true'],\n",
    "         random_forecast['y_predicted'].loc[df_results_full['y_true'].index],average='micro')\n",
    "\n",
    "persistence_forecast = df_week_0.loc[df_results_full['y_true'].index]\n",
    "f1_persistence = f1_score(df_results_full['y_true'],\n",
    "         persistence_forecast['week0'].loc[df_results_full['y_true'].index],average='micro')\n",
    "\n",
    "f1_climatology = f1_score(df_results_full['y_true'],\n",
    "         climatology_forecast['y_predicted'].loc[df_results_full['y_true'].index],average='micro')\n",
    "print(f'F1 XGBoost - {npc} PCs:', f1_results)\n",
    "print('F1 Random:',f1_random)\n",
    "print('F1 Persistence:',f1_persistence)\n",
    "print('F1 Climatology:',f1_climatology)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7a9d2220-3e62-4fd9-8c44-7349fed3002b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## CONCLUDED THAT IT IS BEST TO INCLUDE DAY OF THE YEAR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be904522-f716-4ea5-8d41-45c4233886cc",
   "metadata": {},
   "source": [
    "# Smoothing variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "cf0fe537-038b-4693-a5a2-0bc001cd8042",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# With day of year\n",
    "npc = 10\n",
    "combined_df = pd.read_csv(f'../PC_Data/PCs/PCs_{list_vars[ivar]}.csv',\n",
    "                          index_col=0,\n",
    "                          parse_dates=True)\n",
    "combined_df = combined_df.iloc[:,:npc]\n",
    "\n",
    "# Smoothing\n",
    "combined_df = combined_df.rolling(6).mean()\n",
    "\n",
    "# Normalize day of the year using sine and cosine transformations\n",
    "combined_df['day_sin'] = np.sin(2 * np.pi * combined_df.index.day_of_year / 365)\n",
    "combined_df['day_cos'] = np.cos(2 * np.pi * combined_df.index.day_of_year / 365)\n",
    "\n",
    "fully_combined_df = pd.concat([combined_df,df_shifts[week_out_str]],axis=1)\n",
    "fully_combined_df = fully_combined_df.dropna()\n",
    "dic_trainval, dic_test = get_train_val_test_periods(fully_combined_df)\n",
    "start_of_test_periods = np.arange(1981,2021,10)\n",
    "list_results = []\n",
    "\n",
    "for iperiod in range(len(start_of_test_periods)):\n",
    "    X_trainval = dic_trainval[start_of_test_periods[iperiod]].iloc[:,:-1].values\n",
    "    y_trainval = dic_trainval[start_of_test_periods[iperiod]].iloc[:,-1]\n",
    "\n",
    "    X_test = dic_test[start_of_test_periods[iperiod]].iloc[:,:-1].values\n",
    "    y_test = dic_test[start_of_test_periods[iperiod]].iloc[:,-1]\n",
    "    \n",
    "    cw = class_weight.compute_sample_weight(\n",
    "        class_weight='balanced',\n",
    "        y=y_trainval\n",
    "    )\n",
    "    model = xgb.XGBClassifier(n_estimators=100,\n",
    "                            max_depth=hyperparams['max_depth'],\n",
    "                            learning_rate=hyperparams['learning_rate'],\n",
    "                            subsample=hyperparams['subsample'],\n",
    "                            colsample_bytree=hyperparams['colsample_bytree'],\n",
    "                            # colsample_bylevel=best_params['colsample_bylevel'],\n",
    "                            gamma=hyperparams['gamma'],\n",
    "                            reg_alpha=hyperparams['reg_alpha'],\n",
    "                            reg_lambda=hyperparams['reg_lambda'],\n",
    "                            num_class=5,\n",
    "                            objective = \"multi:softprob\",\n",
    "                            tree_method='hist',\n",
    "                            device = f'cuda:{gpu_id}')\n",
    "    model.fit(X_trainval, y_trainval, sample_weight=cw)\n",
    "    y_predicted = model.predict(X_test)\n",
    "    df_results_temp = pd.DataFrame(np.array([y_test.values,y_predicted]).T,\n",
    "                                   index=y_test.index,\n",
    "                                   columns=['y_true','y_predicted'])\n",
    "    list_results.append(df_results_temp)\n",
    "    \n",
    "df_results_full = pd.concat(list_results,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0de7ddc0-2ca7-4797-8d98-d1515bc0b01a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 XGBoost - 10 PCs: 0.24102193299590263\n",
      "F1 Random: 0.20294046758255002\n",
      "F1 Persistence: 0.21740178356230416\n",
      "F1 Climatology: 0.20607375271149675\n"
     ]
    }
   ],
   "source": [
    "f1_results = f1_score(df_results_full['y_true'],df_results_full['y_predicted'],average='micro')\n",
    "\n",
    "f1_random = f1_score(df_results_full['y_true'],\n",
    "         random_forecast['y_predicted'].loc[df_results_full['y_true'].index],average='micro')\n",
    "\n",
    "persistence_forecast = df_week_0.loc[df_results_full['y_true'].index]\n",
    "f1_persistence = f1_score(df_results_full['y_true'],\n",
    "         persistence_forecast['week0'].loc[df_results_full['y_true'].index],average='micro')\n",
    "\n",
    "f1_climatology = f1_score(df_results_full['y_true'],\n",
    "         climatology_forecast['y_predicted'].loc[df_results_full['y_true'].index],average='micro')\n",
    "print(f'F1 XGBoost - {npc} PCs:', f1_results)\n",
    "print('F1 Random:',f1_random)\n",
    "print('F1 Persistence:',f1_persistence)\n",
    "print('F1 Climatology:',f1_climatology)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff8b8ff-e639-4126-b894-92b1a07c03f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concluded that smoothing doesn't improve the skill"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:xgb_wr]",
   "language": "python",
   "name": "conda-env-xgb_wr-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
