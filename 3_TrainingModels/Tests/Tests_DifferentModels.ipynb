{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b529883d-7b37-4494-8cac-e2e55aa64bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import copy\n",
    "from datetime import datetime, timedelta\n",
    "# from keras.utils import to_categorical\n",
    "# import visualkeras\n",
    "# import tensorflow as tf\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import brier_score_loss\n",
    "# import optuna\n",
    "# from optuna.samplers import TPESampler\n",
    "# import keras\n",
    "# from keras.callbacks import ModelCheckpoint\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import sys\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import glob \n",
    "\n",
    "\n",
    "from sklearn import datasets, ensemble\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.utils import class_weight\n",
    "import json\n",
    "\n",
    "import xgboost as xgb\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1154a5a6-5265-482b-a2da-24e636d29b64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Dec 23 05:38:41 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA A100-SXM4-80GB          On  |   00000000:01:00.0 Off |                    0 |\n",
      "| N/A   32C    P0             65W /  500W |       9MiB /  81920MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   1  NVIDIA A100-SXM4-80GB          On  |   00000000:41:00.0 Off |                    0 |\n",
      "| N/A   44C    P0            216W /  500W |    1791MiB /  81920MiB |     93%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    1   N/A  N/A    174522      C   ...on/conda-envs/pytorch_wr/bin/python       1778MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Run nvidia-smi to get GPU information\n",
    "os.system('nvidia-smi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ae086bb-2a73-4e5d-9c0f-9c48525ea935",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_id = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d0d57c2-0432-4f67-b8d0-f5995e49f636",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_val_test_periods(full_df):\n",
    "    dic_train_val = {}\n",
    "    dic_test = {}\n",
    "    \n",
    "    start_of_test_periods = np.arange(1981,2021,10)\n",
    "    end_of_test_periods = start_of_test_periods + 9\n",
    "    \n",
    "    for iperiod in range(len(start_of_test_periods)):\n",
    "        df_test_temp = full_df[str(start_of_test_periods[iperiod]):str(end_of_test_periods[iperiod])]\n",
    "        df_trainval_temp = full_df.drop(df_test_temp.index)\n",
    "        \n",
    "        dic_train_val[start_of_test_periods[iperiod]] = df_trainval_temp\n",
    "        dic_test[start_of_test_periods[iperiod]] = df_test_temp\n",
    "    return dic_train_val, dic_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "edfce0a8-3571-42b7-8f35-f6510a2a5424",
   "metadata": {},
   "outputs": [],
   "source": [
    "wr_original_series = pd.read_csv('/glade/work/jhayron/Data4Predictability/WR_Series_vSLtest.csv',\\\n",
    "                index_col=0,names=['week0','dist'],skiprows=1,parse_dates=True)\n",
    "# Rolling window for mode\n",
    "rolling_mode = (\n",
    "    wr_original_series.rolling('7d', center=True,min_periods=7)\n",
    "    .apply(lambda x: x.mode()[0] if not x.mode().empty else float('nan'))\n",
    ").shift(-3)\n",
    "\n",
    "# Rolling window for the count of the mode\n",
    "rolling_mode_count = (\n",
    "    wr_original_series.rolling('7d', center=True,min_periods=7)\n",
    "    .apply(lambda x: (x == x.mode()[0]).sum() if not x.mode().empty else 0)\n",
    ").shift(-3)\n",
    "\n",
    "# If duration of WR during week was less than 4, assing NO WR class\n",
    "rolling_mode.loc[rolling_mode_count['week0']<4,'week0'] = 4\n",
    "wr_series_mode = copy.deepcopy(rolling_mode)\n",
    "time_index = pd.to_datetime(wr_series_mode.index).dayofweek\n",
    "wr_series_mode = wr_series_mode.iloc[time_index.isin([0,3])].dropna()\n",
    "wr_series = copy.deepcopy(wr_series_mode)\n",
    "\n",
    "for wk in range(2,10):\n",
    "    series_temp = copy.deepcopy(wr_series[\"week0\"])\n",
    "    series_temp.index = series_temp.index - timedelta(weeks = wk-1)\n",
    "    series_temp.name = f'week{wk-1}'\n",
    "    if wk==2:\n",
    "        df_shifts = pd.concat([pd.DataFrame(wr_series[\"week0\"]),pd.DataFrame(series_temp)],axis=1)  \n",
    "    else:\n",
    "        df_shifts = pd.concat([df_shifts,pd.DataFrame(series_temp)],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3221fcb8-e398-4cf2-af8f-e89a12368e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_folders = np.sort(glob.glob('/glade/u/home/jhayron/WR_Predictability_v2/1_DataPreparation/PC_Data/PCs/*.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "23cce286-c722-486a-9df4-23179d9038df",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_vars = [list_folders[i].split('/')[-1][4:-4] for i in range(len(list_folders))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0242c757-a77d-4cfa-83a4-598cdff9b939",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 IC_SODA\n",
      "1 IT_SODA\n",
      "2 MLD_SODA\n",
      "3 OHC100_SODA\n",
      "4 OHC200_SODA\n",
      "5 OHC300_SODA\n",
      "6 OHC50_SODA\n",
      "7 OHC700_SODA\n",
      "8 OLR_ERA5\n",
      "9 SD_ERA5\n",
      "10 SSH_SODA\n",
      "11 SST_OISSTv2\n",
      "12 SST_SODA\n",
      "13 STL_1m_ERA5\n",
      "14 STL_28cm_ERA5\n",
      "15 STL_7cm_ERA5\n",
      "16 STL_full_ERA5\n",
      "17 SWVL_1m_ERA5\n",
      "18 SWVL_28cm_ERA5\n",
      "19 SWVL_7cm_ERA5\n",
      "20 SWVL_full_ERA5\n",
      "21 U10_ERA5\n",
      "22 U200_ERA5\n",
      "23 Z500_ERA5\n"
     ]
    }
   ],
   "source": [
    "for ivar,var in enumerate(list_vars):\n",
    "    print(ivar,var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a93c0b4e-451a-4592-8d70-4506eb60e37a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'SST_OISSTv2'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ivar = 11\n",
    "list_vars[ivar]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5192f326-aac3-4b9e-ad36-ce37a1b6dee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparams = {\n",
    "    \"n_estimators\": 100,       # Number of boosting rounds, balances speed and depth\n",
    "    \"max_depth\": 6,            # Controls tree depth; deeper trees model complexity better but risk overfitting\n",
    "    \"learning_rate\": 0.1,      # Step size shrinkage for reducing overfitting risk\n",
    "    \"subsample\": 0.8,          # Fraction of training samples used per tree, helps generalize\n",
    "    \"colsample_bytree\": 0.8,   # Fraction of features used per tree, improves robustness\n",
    "    \"gamma\": 0,                # Minimum loss reduction to make a split, prevents overfitting\n",
    "    \"min_child_weight\": 1,     # Minimum sum of instance weight needed in a leaf, controls complexity\n",
    "    \"reg_alpha\": 0,            # L1 regularization term for weights, often fine at 0 unless high sparsity is needed\n",
    "    \"reg_lambda\": 1            # L2 regularization term for weights, controls overfitting\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c1fe1950-3fe2-405c-a4af-c908be3b6346",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "week_out = 4\n",
    "print(week_out)\n",
    "week_out_str = f'week{week_out}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e2905615-09d9-4821-a08f-3e57fb7de13c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SST_OISSTv2\n"
     ]
    }
   ],
   "source": [
    "print(list_vars[ivar])\n",
    "\n",
    "path_weekly_anoms = '/glade/derecho/scratch/jhayron/Data4Predictability/WeeklyAnoms_DetrendedStd_v3_2dg/'\n",
    "path_nc_anoms = f'{path_weekly_anoms}{list_vars[ivar]}.nc'\n",
    "anoms = xr.open_dataset(path_nc_anoms)\n",
    "anoms = anoms.assign_coords(time=pd.DatetimeIndex(anoms.time).normalize())\n",
    "var_name_nc = list(anoms.data_vars.keys())[0]\n",
    "\n",
    "# Define a boxcar filter function\n",
    "def boxcar_filter(data, size):\n",
    "    kernel = np.ones((size, size)) / (size * size)\n",
    "    from scipy.signal import convolve2d\n",
    "    return convolve2d(data, kernel, mode=\"same\", boundary=\"fill\", fillvalue=np.nan)\n",
    "\n",
    "# Apply boxcar filter\n",
    "smoothed_anoms = xr.apply_ufunc(\n",
    "    boxcar_filter,\n",
    "    anoms,\n",
    "    kwargs={\"size\": 3},  # Adjust window size (e.g., 5x5 grid cells)\n",
    "    input_core_dims=[[\"lat\", \"lon\"]],\n",
    "    output_core_dims=[[\"lat\", \"lon\"]],\n",
    "    vectorize=True,\n",
    ")\n",
    "\n",
    "anoms_flattened = smoothed_anoms[var_name_nc].stack(flat_spatial=('lat', 'lon'))\n",
    "anoms_flattened_og = copy.deepcopy(anoms_flattened)\n",
    "# anoms_flattened_og.data[:,anoms_flattened.columns] = anoms_flattened.values WITH THIS LINES I CAN COME BACK\n",
    "# anoms_flattened_og.unstack('flat_spatial') WITH THIS LINES I CAN COME BACK\n",
    "anoms_flattened = pd.DataFrame(anoms_flattened,index = anoms_flattened.time)\n",
    "anoms_flattened = anoms_flattened.dropna(axis=1, how='any')\n",
    "\n",
    "combined_df = copy.deepcopy(anoms_flattened)\n",
    "# combined_df['day_sin'] = np.sin(2 * np.pi * combined_df.index.day_of_year / 365)\n",
    "# combined_df['day_cos'] = np.cos(2 * np.pi * combined_df.index.day_of_year / 365)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a2130f83-88c3-44c0-8356-2a1169a2b94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fully_combined_df = pd.concat([combined_df,df_shifts[week_out_str]],axis=1)\n",
    "fully_combined_df = fully_combined_df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34619768-987b-4366-9509-2cc6508774ed",
   "metadata": {},
   "source": [
    "# Create benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c4fa7bd8-beaf-4d93-a2ed-a8cf8c0e9dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_week_0 = df_shifts[['week0']].dropna()\n",
    "\n",
    "def generate_random_forecast(df_week_0, seed_value=42):\n",
    "    # Set the random seed for reproducibility\n",
    "    np.random.seed(seed_value)\n",
    "    \n",
    "    # Step 1: Get unique classes and their frequencies\n",
    "    values = df_week_0['week0'].value_counts()\n",
    "    \n",
    "    # Step 2: Calculate the probabilities for each class\n",
    "    classes = values.index  # Unique classes\n",
    "    probabilities = values / values.sum()  # Normalize to get probability distribution\n",
    "    \n",
    "    # Step 3: Generate a random forecast based on the probabilities\n",
    "    random_forecast = np.random.choice(classes, size=len(df_week_0), p=probabilities)\n",
    "    \n",
    "    # Step 4: Return the random forecast as a DataFrame or Series\n",
    "    forecast_df = pd.DataFrame(random_forecast, index=df_week_0.index, columns=['y_predicted'])\n",
    "    \n",
    "    return forecast_df\n",
    "\n",
    "def generate_random_forecast_probabilities(df_week_0, seed_value=42):\n",
    "    # Set the random seed for reproducibility\n",
    "    np.random.seed(seed_value)\n",
    "    \n",
    "    # Step 1: Get unique classes and their frequencies\n",
    "    values = df_week_0['week0'].value_counts()\n",
    "    \n",
    "    # Step 2: Calculate the probabilities for each class\n",
    "    classes = values.index  # Unique classes\n",
    "    probabilities = values / values.sum()  # Normalize to get probability distribution\n",
    "    \n",
    "    # Step 3: Create a probability forecast for each sample\n",
    "    # Create a 2D array where each row is the same probability distribution\n",
    "    prob_matrix = np.tile(probabilities.values, (len(df_week_0), 1))\n",
    "    \n",
    "    # Step 4: Return the probability matrix as a DataFrame\n",
    "    forecast_df = pd.DataFrame(prob_matrix, index=df_week_0.index, columns=classes)[np.arange(5)]\n",
    "    \n",
    "    return forecast_df\n",
    "\n",
    "def generate_random_forecast_with_monthly_probabilities(df_week_0, seed_value=42):\n",
    "    # Set the random seed for reproducibility\n",
    "    np.random.seed(seed_value)\n",
    "    \n",
    "    # Extract the month from the index (assuming the index is a datetime index)\n",
    "    df_week_0['month'] = df_week_0.index.month\n",
    "    \n",
    "    # Prepare an empty list to store the random forecast\n",
    "    forecasts = []\n",
    "    \n",
    "    # Loop through each month\n",
    "    for month in range(1, 13):  # Loop through months 1 to 12\n",
    "        # Filter data for the current month\n",
    "        month_data = df_week_0[df_week_0['month'] == month]\n",
    "        \n",
    "        # Step 1: Get unique classes and their frequencies for the current month\n",
    "        values = month_data['week0'].value_counts()\n",
    "        \n",
    "        # Step 2: Calculate the probabilities for each class in the current month\n",
    "        classes = values.index  # Unique classes\n",
    "        probabilities = values / values.sum()  # Normalize to get probability distribution\n",
    "        \n",
    "        # Step 3: Generate random forecasts for the current month based on the probabilities\n",
    "        month_forecast = np.random.choice(classes, size=len(month_data), p=probabilities)\n",
    "        \n",
    "        # Store the forecast for the current month\n",
    "        forecasts.append(pd.Series(month_forecast, index=month_data.index))\n",
    "    \n",
    "    # Combine all monthly forecasts into one DataFrame\n",
    "    forecast_df = pd.concat(forecasts)\n",
    "    forecast_df = forecast_df.sort_index()  # Sort the index to preserve the original order\n",
    "    forecast_df = pd.DataFrame(forecast_df,columns=['y_predicted'])\n",
    "    return forecast_df\n",
    "    \n",
    "def generate_probability_forecast_with_monthly_probabilities(df_week_0, seed_value=42):\n",
    "    # Set the random seed for reproducibility\n",
    "    np.random.seed(seed_value)\n",
    "    \n",
    "    # Extract the month from the index (assuming the index is a datetime index)\n",
    "    df_week_0['month'] = df_week_0.index.month\n",
    "    \n",
    "    # Prepare an empty DataFrame to store the probability forecasts\n",
    "    all_probabilities = pd.DataFrame(index=df_week_0.index)\n",
    "    \n",
    "    # Loop through each month\n",
    "    for month in range(1, 13):  # Loop through months 1 to 12\n",
    "        # Filter data for the current month\n",
    "        month_data = df_week_0[df_week_0['month'] == month]\n",
    "        \n",
    "        if month_data.empty:\n",
    "            continue  # Skip if there's no data for the month\n",
    "        \n",
    "        # Step 1: Get unique classes and their frequencies for the current month\n",
    "        values = month_data['week0'].value_counts()\n",
    "        \n",
    "        # Step 2: Calculate the probabilities for each class in the current month\n",
    "        classes = values.index  # Unique classes\n",
    "        probabilities = values / values.sum()  # Normalize to get probability distribution\n",
    "        \n",
    "        # Step 3: Create a probability matrix for the current month\n",
    "        prob_matrix = np.tile(probabilities.values, (len(month_data), 1))\n",
    "        \n",
    "        # Create a DataFrame for this month's probabilities with appropriate columns\n",
    "        month_prob_df = pd.DataFrame(prob_matrix, index=month_data.index, columns=classes)\n",
    "        \n",
    "        # Append this month's DataFrame to the overall probability DataFrame\n",
    "        all_probabilities = pd.concat([all_probabilities, month_prob_df])\n",
    "    \n",
    "    # Sort the index to match the original order\n",
    "    all_probabilities = all_probabilities.sort_index()\n",
    "    \n",
    "    # Fill missing columns with zeros for months that do not include certain classes\n",
    "    all_classes = df_week_0['week0'].unique()\n",
    "    all_probabilities = all_probabilities.reindex(columns=all_classes, fill_value=0).dropna()[np.arange(5)]\n",
    "    \n",
    "    return all_probabilities\n",
    "\n",
    "random_forecast = generate_random_forecast(df_week_0, seed_value=42)\n",
    "climatology_forecast = generate_random_forecast_with_monthly_probabilities(df_week_0, seed_value=42)\n",
    "random_forecast_probs = generate_random_forecast_probabilities(df_week_0)\n",
    "climatology_forecast_probs = generate_probability_forecast_with_monthly_probabilities(df_week_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9176e274-bf97-44dd-9f42-d7f423928860",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "00108790-c024-42bc-a186-e6946a0586fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "fully_combined_df = pd.concat([combined_df,df_shifts[week_out_str]],axis=1)\n",
    "fully_combined_df = fully_combined_df.dropna()\n",
    "dic_trainval, dic_test = get_train_val_test_periods(fully_combined_df)\n",
    "start_of_test_periods = np.arange(1981,2021,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "08ca34e9-28be-4959-b151-d23b7049ab7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/glade/work/jhayron/conda-envs/pytorch_wr/lib/python3.12/site-packages/xgboost/core.py:158: UserWarning: [05:41:31] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1730232887822/work/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
      "Potential solutions:\n",
      "- Use a data structure that matches the device ordinal in the booster.\n",
      "- Set the device for booster before call to inplace_predict.\n",
      "\n",
      "This warning will only be shown once.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "list_results = []\n",
    "list_results_probs = []\n",
    "\n",
    "for iperiod in range(len(start_of_test_periods)):\n",
    "    print(iperiod)\n",
    "    X_trainval = dic_trainval[start_of_test_periods[iperiod]].iloc[:,:-1].values\n",
    "    y_trainval = dic_trainval[start_of_test_periods[iperiod]].iloc[:,-1]\n",
    "\n",
    "    X_test = dic_test[start_of_test_periods[iperiod]].iloc[:,:-1].values\n",
    "    y_test = dic_test[start_of_test_periods[iperiod]].iloc[:,-1]\n",
    "    \n",
    "    cw = class_weight.compute_sample_weight(\n",
    "        class_weight='balanced',\n",
    "        y=y_trainval\n",
    "    )\n",
    "\n",
    "    model = xgb.XGBClassifier(n_estimators=100,\n",
    "                            max_depth=hyperparams['max_depth'],\n",
    "                            learning_rate=hyperparams['learning_rate'],\n",
    "                            subsample=hyperparams['subsample'],\n",
    "                            colsample_bytree=hyperparams['colsample_bytree'],\n",
    "                            # colsample_bylevel=best_params['colsample_bylevel'],\n",
    "                            gamma=hyperparams['gamma'],\n",
    "                            reg_alpha=hyperparams['reg_alpha'],\n",
    "                            reg_lambda=hyperparams['reg_lambda'],\n",
    "                            num_class=5,\n",
    "                            objective = \"multi:softprob\",\n",
    "                            tree_method='hist',\n",
    "                            device = f'cuda:{gpu_id}')\n",
    "    model.fit(X_trainval, y_trainval, sample_weight=cw)\n",
    "    y_predicted = model.predict(X_test)\n",
    "    y_predicted_probs = model.predict_proba(X_test)\n",
    "    y_predicted_probs = pd.DataFrame(y_predicted_probs,index=y_test.index)\n",
    "    df_results_temp = pd.DataFrame(np.array([y_test.values,y_predicted]).T,\n",
    "                                   index=y_test.index,\n",
    "                                   columns=['y_true','y_predicted'])\n",
    "    list_results.append(df_results_temp)\n",
    "    list_results_probs.append(y_predicted_probs)\n",
    "    \n",
    "df_results_full = pd.concat(list_results,axis=0)\n",
    "df_results_full_probs = pd.concat(list_results_probs,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1f3c7fc0-f847-4da2-9a24-5a71dc86f2f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 XGBoost: 0.2721055202735711\n",
      "F1 Random: 0.21812408402540304\n",
      "F1 Persistence: 0.23326819736199317\n",
      "F1 Climatology: 0.23277967757694187\n"
     ]
    }
   ],
   "source": [
    "f1_results = f1_score(df_results_full['y_true'],df_results_full['y_predicted'],average='micro')\n",
    "\n",
    "f1_random = f1_score(df_results_full['y_true'],\n",
    "         random_forecast['y_predicted'].loc[df_results_full['y_true'].index],average='micro')\n",
    "\n",
    "persistence_forecast = df_week_0.loc[df_results_full['y_true'].index]\n",
    "f1_persistence = f1_score(df_results_full['y_true'],\n",
    "         persistence_forecast['week0'].loc[df_results_full['y_true'].index],average='micro')\n",
    "\n",
    "f1_climatology = f1_score(df_results_full['y_true'],\n",
    "         climatology_forecast['y_predicted'].loc[df_results_full['y_true'].index],average='micro')\n",
    "print(f'F1 XGBoost:', f1_results)\n",
    "print('F1 Random:',f1_random)\n",
    "print('F1 Persistence:',f1_persistence)\n",
    "print('F1 Climatology:',f1_climatology)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d4e4e7-3104-40be-ae2e-3bfbfe288b1c",
   "metadata": {},
   "source": [
    "# ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "554b6def-417a-44d7-be14-a122ec1a352d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2df0d4f6-edcb-472d-8ec6-1ccdfb2e8243",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fully_combined_df = pd.concat([combined_df,df_shifts[week_out_str]],axis=1)\n",
    "fully_combined_df = fully_combined_df.dropna()\n",
    "dic_trainval, dic_test = get_train_val_test_periods(fully_combined_df)\n",
    "start_of_test_periods = np.arange(1981,2021,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5178c09f-997a-49de-88f8-d51e18165036",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define general-purpose ANN model\n",
    "class GeneralANN(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        self.do = 0.3\n",
    "        super(GeneralANN, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_size, 1024),      # First hidden layer with fewer neurons\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.Dropout(self.do),                  # Dropout for regularization\n",
    "            \n",
    "            nn.Linear(1024, 512),             # Second hidden layer\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.Dropout(self.do),\n",
    "            \n",
    "            nn.Linear(512, 256),              # Third hidden layer (optional, depending on needs)\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.Dropout(self.do),\n",
    "\n",
    "            nn.Linear(256, 64),              # Fourth hidden layer (optional, depending on needs)\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.Dropout(self.do),\n",
    "            \n",
    "            nn.Linear(64, num_classes)       # Output layer for classification\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "        \n",
    "def build_train_evaluate_ann(X_trainval,y_trainval,X_test,y_test):\n",
    "    # Split the dataset into training and validation sets\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_trainval, y_trainval, test_size=0.2, shuffle=False)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # Convert split data to PyTorch tensors\n",
    "    X_train_tensor = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "    y_train_tensor = torch.tensor(y_train.values, dtype=torch.long).to(device)\n",
    "    X_val_tensor = torch.tensor(X_val, dtype=torch.float32).to(device)\n",
    "    y_val_tensor = torch.tensor(y_val.values, dtype=torch.long).to(device)\n",
    "    X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "    y_test_tensor = torch.tensor(y_test.values, dtype=torch.long).to(device)\n",
    "    \n",
    "    # Create datasets and dataloaders\n",
    "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "    \n",
    "    batch_size = 128\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Initialize model, loss function, and optimizer\n",
    "    input_size = X_trainval.shape[1]      # Number of features\n",
    "    num_classes = 5         # Adjust based on your classification task\n",
    "    model = GeneralANN(input_size, num_classes)\n",
    "    \n",
    "    # Move model to GPU if available\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss() # Appropriate for multi-class classification\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.00001)\n",
    "    \n",
    "    # Set random seed for reproducibility\n",
    "    torch.manual_seed(42)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(42)\n",
    "        \n",
    "    # Early stopping parameters\n",
    "    epochs = 50\n",
    "    patience = 5\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    \n",
    "    # Training loop with early stopping\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "        \n",
    "        # Average losses\n",
    "        train_loss = running_loss / len(train_loader)\n",
    "        val_loss = val_loss / len(val_loader)\n",
    "        # print(f'Epoch {epoch+1}, Train Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}')\n",
    "        \n",
    "        # Check for early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), 'ann_best_model.pth')  # Save the best model\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                # print(\"Early stopping triggered\")\n",
    "                break\n",
    "    \n",
    "    # Load the best model after training\n",
    "    model.load_state_dict(torch.load('ann_best_model.pth'))\n",
    "    os.remove('ann_best_model.pth')\n",
    "    y_predicted = model(X_test_tensor)\n",
    "    probabilities = torch.softmax(y_predicted, dim=1)\n",
    "    y_predicted = torch.argmax(y_predicted, dim=1)\n",
    "    \n",
    "    results_temp = pd.DataFrame(y_predicted.cpu().detach().numpy(), index=y_test.index, columns=['y_predicted'])\n",
    "    results_temp_probabilities = pd.DataFrame(probabilities.cpu().detach().numpy(), index=y_test.index)\n",
    "    return results_temp, results_temp_probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fbeda603-1be1-4ec1-b53d-d7398aabcd61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/glade/derecho/scratch/jhayron/tmp/ipykernel_174869/3264798996.py:115: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('ann_best_model.pth'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "list_results = []\n",
    "list_results_probs = []\n",
    "\n",
    "for iperiod in range(len(start_of_test_periods)):\n",
    "    print(iperiod)\n",
    "    X_trainval = dic_trainval[start_of_test_periods[iperiod]].iloc[:,:-1].values\n",
    "    y_trainval = dic_trainval[start_of_test_periods[iperiod]].iloc[:,-1]\n",
    "\n",
    "    X_test = dic_test[start_of_test_periods[iperiod]].iloc[:,:-1].values\n",
    "    y_test = dic_test[start_of_test_periods[iperiod]].iloc[:,-1]\n",
    "\n",
    "    \n",
    "    y_predicted, y_predicted_probs = build_train_evaluate_ann(X_trainval,y_trainval,X_test,y_test)\n",
    "    df_results_temp = pd.DataFrame(np.array([y_test.values,y_predicted['y_predicted'].values]).T,\n",
    "                                   index=y_test.index,\n",
    "                                   columns=['y_true','y_predicted'])\n",
    "    list_results.append(df_results_temp)\n",
    "    list_results_probs.append(y_predicted_probs)\n",
    "    \n",
    "df_results_full = pd.concat(list_results,axis=0)\n",
    "df_results_full_probs = pd.concat(list_results_probs,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f3f7a4a1-46ff-4605-bd0a-b6ddd072e78c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 ANN: 0.2122618466047875\n",
      "F1 Random: 0.21812408402540304\n",
      "F1 Persistence: 0.23326819736199317\n",
      "F1 Climatology: 0.23277967757694187\n"
     ]
    }
   ],
   "source": [
    "f1_results = f1_score(df_results_full['y_true'],df_results_full['y_predicted'],average='micro')\n",
    "\n",
    "f1_random = f1_score(df_results_full['y_true'],\n",
    "         random_forecast['y_predicted'].loc[df_results_full['y_true'].index],average='micro')\n",
    "\n",
    "persistence_forecast = df_week_0.loc[df_results_full['y_true'].index]\n",
    "f1_persistence = f1_score(df_results_full['y_true'],\n",
    "         persistence_forecast['week0'].loc[df_results_full['y_true'].index],average='micro')\n",
    "\n",
    "f1_climatology = f1_score(df_results_full['y_true'],\n",
    "         climatology_forecast['y_predicted'].loc[df_results_full['y_true'].index],average='micro')\n",
    "print(f'F1 ANN:', f1_results)\n",
    "print('F1 Random:',f1_random)\n",
    "print('F1 Persistence:',f1_persistence)\n",
    "print('F1 Climatology:',f1_climatology)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3cfbc00-19d3-4f3f-a088-8a660f3f1df9",
   "metadata": {},
   "source": [
    "# ANN Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c8818393-4287-43a0-8a92-58cf7599dab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c19ae570-bf61-488c-8379-e6a55f61305d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fully_combined_df = pd.concat([combined_df,df_shifts[week_out_str]],axis=1)\n",
    "fully_combined_df = fully_combined_df.dropna()\n",
    "dic_trainval, dic_test = get_train_val_test_periods(fully_combined_df)\n",
    "start_of_test_periods = np.arange(1981,2021,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7b38878d-bab0-48c7-b272-b710d0285b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define general-purpose ANN model\n",
    "class GeneralANN(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        self.do = 0.3\n",
    "        super(GeneralANN, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_size, 1024),      # First hidden layer with fewer neurons\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.Dropout(self.do),                  # Dropout for regularization\n",
    "            \n",
    "            nn.Linear(1024, 512),             # Second hidden layer\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.Dropout(self.do),\n",
    "            \n",
    "            nn.Linear(512, 256),              # Third hidden layer (optional, depending on needs)\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.Dropout(self.do),\n",
    "\n",
    "            nn.Linear(256, 64),              # Fourth hidden layer (optional, depending on needs)\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.Dropout(self.do),\n",
    "            \n",
    "            nn.Linear(64, num_classes)       # Output layer for classification\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "        \n",
    "def build_train_evaluate_ann(X_trainval,y_trainval,X_test,y_test):\n",
    "    # Split the dataset into training and validation sets\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_trainval, y_trainval, test_size=0.2, shuffle=False)\n",
    "    cw_classes = compute_class_weight(class_weight='balanced', classes=np.arange(5), y=y_trainval)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    cw_classes_tensor = torch.tensor(cw_classes, dtype=torch.float32, device=device)\n",
    "    \n",
    "    # Convert split data to PyTorch tensors\n",
    "    X_train_tensor = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "    y_train_tensor = torch.tensor(y_train.values, dtype=torch.long).to(device)\n",
    "    X_val_tensor = torch.tensor(X_val, dtype=torch.float32).to(device)\n",
    "    y_val_tensor = torch.tensor(y_val.values, dtype=torch.long).to(device)\n",
    "    X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "    y_test_tensor = torch.tensor(y_test.values, dtype=torch.long).to(device)\n",
    "    \n",
    "    # Create datasets and dataloaders\n",
    "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "    \n",
    "    batch_size = 128\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Initialize model, loss function, and optimizer\n",
    "    input_size = X_trainval.shape[1]      # Number of features\n",
    "    num_classes = 5         # Adjust based on your classification task\n",
    "    model = GeneralANN(input_size, num_classes)\n",
    "    \n",
    "    # Move model to GPU if available\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss(weight = cw_classes_tensor) # Appropriate for multi-class classification\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.00001)\n",
    "    \n",
    "    # Set random seed for reproducibility\n",
    "    torch.manual_seed(42)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(42)\n",
    "        \n",
    "    # Early stopping parameters\n",
    "    epochs = 50\n",
    "    patience = 5\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    \n",
    "    # Training loop with early stopping\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "        \n",
    "        # Average losses\n",
    "        train_loss = running_loss / len(train_loader)\n",
    "        val_loss = val_loss / len(val_loader)\n",
    "        # print(f'Epoch {epoch+1}, Train Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}')\n",
    "        \n",
    "        # Check for early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), 'ann_best_model.pth')  # Save the best model\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                # print(\"Early stopping triggered\")\n",
    "                break\n",
    "    \n",
    "    # Load the best model after training\n",
    "    model.load_state_dict(torch.load('ann_best_model.pth'))\n",
    "    os.remove('ann_best_model.pth')\n",
    "    y_predicted = model(X_test_tensor)\n",
    "    probabilities = torch.softmax(y_predicted, dim=1)\n",
    "    y_predicted = torch.argmax(y_predicted, dim=1)\n",
    "    \n",
    "    results_temp = pd.DataFrame(y_predicted.cpu().detach().numpy(), index=y_test.index, columns=['y_predicted'])\n",
    "    results_temp_probabilities = pd.DataFrame(probabilities.cpu().detach().numpy(), index=y_test.index)\n",
    "    return results_temp, results_temp_probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "59c1606d-fef9-478c-a525-85270bfd88a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/glade/derecho/scratch/jhayron/tmp/ipykernel_174869/416893788.py:119: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('ann_best_model.pth'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/glade/derecho/scratch/jhayron/tmp/ipykernel_174869/416893788.py:119: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('ann_best_model.pth'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/glade/derecho/scratch/jhayron/tmp/ipykernel_174869/416893788.py:119: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('ann_best_model.pth'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/glade/derecho/scratch/jhayron/tmp/ipykernel_174869/416893788.py:119: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('ann_best_model.pth'))\n"
     ]
    }
   ],
   "source": [
    "list_results = []\n",
    "list_results_probs = []\n",
    "\n",
    "for iperiod in range(len(start_of_test_periods)):\n",
    "    print(iperiod)\n",
    "    X_trainval = dic_trainval[start_of_test_periods[iperiod]].iloc[:,:-1].values\n",
    "    y_trainval = dic_trainval[start_of_test_periods[iperiod]].iloc[:,-1]\n",
    "\n",
    "    X_test = dic_test[start_of_test_periods[iperiod]].iloc[:,:-1].values\n",
    "    y_test = dic_test[start_of_test_periods[iperiod]].iloc[:,-1]\n",
    "\n",
    "    \n",
    "    y_predicted, y_predicted_probs = build_train_evaluate_ann(X_trainval,y_trainval,X_test,y_test)\n",
    "    df_results_temp = pd.DataFrame(np.array([y_test.values,y_predicted['y_predicted'].values]).T,\n",
    "                                   index=y_test.index,\n",
    "                                   columns=['y_true','y_predicted'])\n",
    "    list_results.append(df_results_temp)\n",
    "    list_results_probs.append(y_predicted_probs)\n",
    "    \n",
    "df_results_full = pd.concat(list_results,axis=0)\n",
    "df_results_full_probs = pd.concat(list_results_probs,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "395962b2-3553-44e6-9748-95a396b34480",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 ANN: 0.22716170004885197\n",
      "F1 Random: 0.21812408402540304\n",
      "F1 Persistence: 0.23326819736199317\n",
      "F1 Climatology: 0.23277967757694187\n"
     ]
    }
   ],
   "source": [
    "f1_results = f1_score(df_results_full['y_true'],df_results_full['y_predicted'],average='micro')\n",
    "\n",
    "f1_random = f1_score(df_results_full['y_true'],\n",
    "         random_forecast['y_predicted'].loc[df_results_full['y_true'].index],average='micro')\n",
    "\n",
    "persistence_forecast = df_week_0.loc[df_results_full['y_true'].index]\n",
    "f1_persistence = f1_score(df_results_full['y_true'],\n",
    "         persistence_forecast['week0'].loc[df_results_full['y_true'].index],average='micro')\n",
    "\n",
    "f1_climatology = f1_score(df_results_full['y_true'],\n",
    "         climatology_forecast['y_predicted'].loc[df_results_full['y_true'].index],average='micro')\n",
    "print(f'F1 ANN:', f1_results)\n",
    "print('F1 Random:',f1_random)\n",
    "print('F1 Persistence:',f1_persistence)\n",
    "print('F1 Climatology:',f1_climatology)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcdf89d6-d968-48eb-be6b-5ace8b583770",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ec592899-b147-4805-95cd-6842086c6fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import torch\n",
    "import torch.nn as nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9f71fde8-15a8-41a4-bdaa-804fc735c17f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lagged_dataset(X, y, num_lags=4, skip=1):\n",
    "    \"\"\"\n",
    "    Creates a dataset with lagged data for LSTM training.\n",
    "    X: Original feature array (samples, features).\n",
    "    y: Original target array (samples,).\n",
    "    num_lags: Number of lags to include (how many previous time steps).\n",
    "    skip: The step size to skip between lags (e.g., skip=2 means t-8, t-6, t-4, t-2, t=0).\n",
    "    Returns: X_lagged (samples, timesteps, features), y_lagged (samples,).\n",
    "    \"\"\"\n",
    "    samples, features = X.shape\n",
    "    X_lagged = []\n",
    "    y_lagged = []\n",
    "    \n",
    "    # Create sequences with lagged data considering skip\n",
    "    for i in range(num_lags * skip, samples):\n",
    "        # Select data points based on the skip factor\n",
    "        lagged_sequence = X[i - num_lags * skip:i:skip]  # (num_lags, features)\n",
    "        lagged_sequence = np.vstack([lagged_sequence, X[i]])  # Add t=0 at the end\n",
    "        X_lagged.append(lagged_sequence)\n",
    "        y_lagged.append(y[i])  # Target corresponds to t=0\n",
    "    \n",
    "    # Convert to arrays with the shape (samples, timesteps, features)\n",
    "    X_lagged = np.array(X_lagged)\n",
    "    y_lagged = np.array(y_lagged)\n",
    "    \n",
    "    return X_lagged, y_lagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7fd2a6d3-a189-4dd8-be70-0e34ed0dac66",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes, num_lstm_layers=2, dropout=0.3):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        \n",
    "        # Define the LSTM layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_lstm_layers, batch_first=True, dropout=dropout)\n",
    "        \n",
    "        # Fully connected layer to map LSTM outputs to class scores\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "        \n",
    "        # Dropout layer for regularization\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Forward pass through LSTM\n",
    "        lstm_out, (hn, cn) = self.lstm(x)\n",
    "        \n",
    "        # We use the output of the last time step (t=0 in this case)\n",
    "        last_time_step_output = lstm_out[:, -1, :]\n",
    "        \n",
    "        # Pass the output through the fully connected layer\n",
    "        out = self.fc(last_time_step_output)\n",
    "        \n",
    "        # Apply dropout regularization\n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c26b7634-4447-4644-8f72-98fec1cdcb59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_train_evaluate_lstm(X_trainval,y_trainval,X_test,y_test):\n",
    "    # Create the lagged dataset\n",
    "    num_lags = 3\n",
    "    skip = 2\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    X_trainval_lagged, y_trainval_lagged = create_lagged_dataset(X_trainval, y_trainval.values, num_lags=num_lags, skip=skip)\n",
    "    X_test_lagged, y_test_lagged = create_lagged_dataset(X_test, y_test.values, num_lags=num_lags, skip=skip)\n",
    "\n",
    "    # Convert to PyTorch tensors\n",
    "    X_test_tensor = torch.tensor(X_test_lagged, dtype=torch.float32).to(device)\n",
    "    y_test_tensor = torch.tensor(y_test_lagged, dtype=torch.long).to(device)\n",
    "    \n",
    "    # Split into training and validation sets\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_trainval_lagged, y_trainval_lagged, test_size=0.2, shuffle=False)\n",
    "    \n",
    "    # Convert to PyTorch tensors\n",
    "    X_train_tensor = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "    y_train_tensor = torch.tensor(y_train, dtype=torch.long).to(device)\n",
    "    X_val_tensor = torch.tensor(X_val, dtype=torch.float32).to(device)\n",
    "    y_val_tensor = torch.tensor(y_val, dtype=torch.long).to(device)\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "    \n",
    "    # Set random seed for reproducibility\n",
    "    torch.manual_seed(42)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(42)\n",
    "        \n",
    "    # Create data loaders\n",
    "    batch_size = 128\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    # Example initialization and usage:\n",
    "    input_size = X_train_tensor.shape[-1]  # Number of features (size of each input vector)\n",
    "    hidden_size = 512  # Hidden state size of LSTM\n",
    "    num_classes = 5  # Number of classes for classification\n",
    "    num_lstm_layers = 2  # Number of LSTM layers\n",
    "    dropout = 0.2  # Dropout rate\n",
    "    \n",
    "    # Instantiate the model\n",
    "    model = LSTMModel(input_size, hidden_size, num_classes, num_lstm_layers, dropout)\n",
    "    \n",
    "    # Print the model architecture\n",
    "    model = model.to(device)\n",
    "    # Loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()  # Suitable for multi-class classification\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.00001)\n",
    "\n",
    "    # Early stopping parameters\n",
    "    epochs = 50\n",
    "    patience = 5\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    \n",
    "    # Training loop with early stopping\n",
    "    for epoch in range(epochs):\n",
    "        model.train()  # Set model to training mode\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)  # Move to device (GPU/CPU)\n",
    "            \n",
    "            optimizer.zero_grad()  # Zero out the gradients\n",
    "            outputs = model(inputs)  # Get predictions from the model\n",
    "            loss = criterion(outputs, labels)  # Compute loss\n",
    "            loss.backward()  # Backpropagation\n",
    "            optimizer.step()  # Update model weights\n",
    "            \n",
    "            running_loss += loss.item()  # Track loss for this batch\n",
    "    \n",
    "        # Validation phase (no gradients needed)\n",
    "        model.eval()  # Set model to evaluation mode\n",
    "        val_loss = 0.0\n",
    "        y_val_pred = []\n",
    "        y_val_true = []\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)  # Move to device\n",
    "                outputs = model(inputs)  # Get predictions\n",
    "                loss = criterion(outputs, labels)  # Compute loss\n",
    "                val_loss += loss.item()  # Track validation loss\n",
    "                y_val_pred.append(outputs.argmax(dim=1))  # Get predicted class labels\n",
    "                y_val_true.append(labels)  # True labels\n",
    "    \n",
    "        # Average losses\n",
    "        train_loss = running_loss / len(train_loader)\n",
    "        val_loss = val_loss / len(val_loader)\n",
    "    \n",
    "        # Convert validation predictions to a single tensor\n",
    "        y_val_pred = torch.cat(y_val_pred).cpu().numpy()\n",
    "        y_val_true = torch.cat(y_val_true).cpu().numpy()\n",
    "    \n",
    "        # Evaluate F1 score (or any other metric you prefer)\n",
    "        f1 = f1_score(y_val_true, y_val_pred, average='micro')\n",
    "        # print(f\"Epoch {epoch+1}/{epochs} - Train Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}, F1 Score: {f1:.4f}\")\n",
    "    \n",
    "        # Early stopping check\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), 'best_lstm_model.pth')  # Save the best model\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                # print(\"Early stopping triggered\")\n",
    "                break\n",
    "    \n",
    "    # Load the best model after training\n",
    "    model.load_state_dict(torch.load('best_lstm_model.pth'))\n",
    "    os.remove('best_lstm_model.pth')\n",
    "    \n",
    "    y_predicted = model(X_test_tensor)\n",
    "    probabilities = torch.softmax(y_predicted, dim=1)\n",
    "    y_predicted = torch.argmax(y_predicted, dim=1)\n",
    "    \n",
    "    results_temp = pd.DataFrame(y_predicted.cpu().detach().numpy(), \n",
    "                                index=y_test[num_lags*skip:].index, columns=['y_predicted'])\n",
    "    results_temp_probabilities = pd.DataFrame(probabilities.cpu().detach().numpy(),\n",
    "                                              index=y_test[num_lags*skip:].index)\n",
    "    return y_test[num_lags*skip:], results_temp, results_temp_probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e9a28f89-105a-4c30-8ed1-8b1e3412e392",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fully_combined_df = pd.concat([combined_df,df_shifts[week_out_str]],axis=1)\n",
    "fully_combined_df = fully_combined_df.dropna()\n",
    "dic_trainval, dic_test = get_train_val_test_periods(fully_combined_df)\n",
    "start_of_test_periods = np.arange(1981,2021,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bb62fef1-b9e6-4abc-852b-0986123d4e4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/glade/derecho/scratch/jhayron/tmp/ipykernel_174869/3909201925.py:112: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('best_lstm_model.pth'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/glade/derecho/scratch/jhayron/tmp/ipykernel_174869/3909201925.py:112: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('best_lstm_model.pth'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/glade/derecho/scratch/jhayron/tmp/ipykernel_174869/3909201925.py:112: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('best_lstm_model.pth'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/glade/derecho/scratch/jhayron/tmp/ipykernel_174869/3909201925.py:112: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('best_lstm_model.pth'))\n"
     ]
    }
   ],
   "source": [
    "list_results = []\n",
    "list_ytests_modified = []\n",
    "list_results_probs = []\n",
    "\n",
    "for iperiod in range(len(start_of_test_periods)):\n",
    "    print(iperiod)\n",
    "    X_trainval = dic_trainval[start_of_test_periods[iperiod]].iloc[:,:-1].values\n",
    "    y_trainval = dic_trainval[start_of_test_periods[iperiod]].iloc[:,-1]\n",
    "\n",
    "    X_test = dic_test[start_of_test_periods[iperiod]].iloc[:,:-1].values\n",
    "    y_test = dic_test[start_of_test_periods[iperiod]].iloc[:,-1]\n",
    "    \n",
    "    y_test_new, y_predicted, y_predicted_probs  = build_train_evaluate_lstm(X_trainval,y_trainval,\n",
    "                                                        X_test,y_test)\n",
    "    \n",
    "    df_results_temp = pd.DataFrame(np.array([y_test_new.values,y_predicted['y_predicted'].values]).T,\n",
    "                                   index=y_test_new.index,\n",
    "                                   columns=['y_true','y_predicted'])\n",
    "    list_results.append(df_results_temp)\n",
    "    list_results_probs.append(y_predicted_probs)\n",
    "    \n",
    "df_results_full = pd.concat(list_results,axis=0)\n",
    "df_results_full_probs = pd.concat(list_results_probs,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c09e7c5f-13c9-4030-9c30-d60fe33884da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 LSTM: 0.25896805896805897\n",
      "F1 Random: 0.2176904176904177\n",
      "F1 Persistence: 0.2339066339066339\n",
      "F1 Climatology: 0.23267813267813267\n"
     ]
    }
   ],
   "source": [
    "f1_results = f1_score(df_results_full['y_true'],df_results_full['y_predicted'],average='micro')\n",
    "\n",
    "f1_random = f1_score(df_results_full['y_true'],\n",
    "         random_forecast['y_predicted'].loc[df_results_full['y_true'].index],average='micro')\n",
    "\n",
    "persistence_forecast = df_week_0.loc[df_results_full['y_true'].index]\n",
    "f1_persistence = f1_score(df_results_full['y_true'],\n",
    "         persistence_forecast['week0'].loc[df_results_full['y_true'].index],average='micro')\n",
    "\n",
    "f1_climatology = f1_score(df_results_full['y_true'],\n",
    "         climatology_forecast['y_predicted'].loc[df_results_full['y_true'].index],average='micro')\n",
    "print(f'F1 LSTM:', f1_results)\n",
    "print('F1 Random:',f1_random)\n",
    "print('F1 Persistence:',f1_persistence)\n",
    "print('F1 Climatology:',f1_climatology)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc6d140-ef0c-4c1b-9408-520f76e9a142",
   "metadata": {},
   "source": [
    "# LSTM Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "39b71480-aacf-445b-94c2-2cb12c3e7cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import torch\n",
    "import torch.nn as nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a1ba6df0-fbde-4481-a8a6-f832e9367aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lagged_dataset(X, y, num_lags=4, skip=1):\n",
    "    \"\"\"\n",
    "    Creates a dataset with lagged data for LSTM training.\n",
    "    X: Original feature array (samples, features).\n",
    "    y: Original target array (samples,).\n",
    "    num_lags: Number of lags to include (how many previous time steps).\n",
    "    skip: The step size to skip between lags (e.g., skip=2 means t-8, t-6, t-4, t-2, t=0).\n",
    "    Returns: X_lagged (samples, timesteps, features), y_lagged (samples,).\n",
    "    \"\"\"\n",
    "    samples, features = X.shape\n",
    "    X_lagged = []\n",
    "    y_lagged = []\n",
    "    \n",
    "    # Create sequences with lagged data considering skip\n",
    "    for i in range(num_lags * skip, samples):\n",
    "        # Select data points based on the skip factor\n",
    "        lagged_sequence = X[i - num_lags * skip:i:skip]  # (num_lags, features)\n",
    "        lagged_sequence = np.vstack([lagged_sequence, X[i]])  # Add t=0 at the end\n",
    "        X_lagged.append(lagged_sequence)\n",
    "        y_lagged.append(y[i])  # Target corresponds to t=0\n",
    "    \n",
    "    # Convert to arrays with the shape (samples, timesteps, features)\n",
    "    X_lagged = np.array(X_lagged)\n",
    "    y_lagged = np.array(y_lagged)\n",
    "    \n",
    "    return X_lagged, y_lagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "db6e2cb6-943b-44e6-b776-e082e9050118",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes, num_lstm_layers=2, dropout=0.3):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        \n",
    "        # Define the LSTM layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_lstm_layers, batch_first=True, dropout=dropout)\n",
    "        \n",
    "        # Fully connected layer to map LSTM outputs to class scores\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "        \n",
    "        # Dropout layer for regularization\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Forward pass through LSTM\n",
    "        lstm_out, (hn, cn) = self.lstm(x)\n",
    "        \n",
    "        # We use the output of the last time step (t=0 in this case)\n",
    "        last_time_step_output = lstm_out[:, -1, :]\n",
    "        \n",
    "        # Pass the output through the fully connected layer\n",
    "        out = self.fc(last_time_step_output)\n",
    "        \n",
    "        # Apply dropout regularization\n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "24e23c54-28d7-4a74-bd52-52acaa7fac0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_train_evaluate_lstm(X_trainval,y_trainval,X_test,y_test):\n",
    "    # Create the lagged dataset\n",
    "    num_lags = 3\n",
    "    skip = 2\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    cw_classes = compute_class_weight(class_weight='balanced', classes=np.arange(5), y=y_trainval)\n",
    "    cw_classes_tensor = torch.tensor(cw_classes, dtype=torch.float32, device=device)\n",
    "    \n",
    "    X_trainval_lagged, y_trainval_lagged = create_lagged_dataset(X_trainval, y_trainval.values, num_lags=num_lags, skip=skip)\n",
    "    X_test_lagged, y_test_lagged = create_lagged_dataset(X_test, y_test.values, num_lags=num_lags, skip=skip)\n",
    "\n",
    "    # Convert to PyTorch tensors\n",
    "    X_test_tensor = torch.tensor(X_test_lagged, dtype=torch.float32).to(device)\n",
    "    y_test_tensor = torch.tensor(y_test_lagged, dtype=torch.long).to(device)\n",
    "    \n",
    "    # Split into training and validation sets\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_trainval_lagged, y_trainval_lagged, test_size=0.2, shuffle=False)\n",
    "    \n",
    "    # Convert to PyTorch tensors\n",
    "    X_train_tensor = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "    y_train_tensor = torch.tensor(y_train, dtype=torch.long).to(device)\n",
    "    X_val_tensor = torch.tensor(X_val, dtype=torch.float32).to(device)\n",
    "    y_val_tensor = torch.tensor(y_val, dtype=torch.long).to(device)\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "    \n",
    "    # Set random seed for reproducibility\n",
    "    torch.manual_seed(42)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(42)\n",
    "        \n",
    "    # Create data loaders\n",
    "    batch_size = 128\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    # Example initialization and usage:\n",
    "    input_size = X_train_tensor.shape[-1]  # Number of features (size of each input vector)\n",
    "    hidden_size = 512  # Hidden state size of LSTM\n",
    "    num_classes = 5  # Number of classes for classification\n",
    "    num_lstm_layers = 2  # Number of LSTM layers\n",
    "    dropout = 0.2  # Dropout rate\n",
    "    \n",
    "    # Instantiate the model\n",
    "    model = LSTMModel(input_size, hidden_size, num_classes, num_lstm_layers, dropout)\n",
    "    \n",
    "    # Print the model architecture\n",
    "    model = model.to(device)\n",
    "    # Loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss(weight = cw_classes_tensor)  # Suitable for multi-class classification\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.00001)\n",
    "\n",
    "    # Early stopping parameters\n",
    "    epochs = 50\n",
    "    patience = 5\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    \n",
    "    # Training loop with early stopping\n",
    "    for epoch in range(epochs):\n",
    "        model.train()  # Set model to training mode\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)  # Move to device (GPU/CPU)\n",
    "            \n",
    "            optimizer.zero_grad()  # Zero out the gradients\n",
    "            outputs = model(inputs)  # Get predictions from the model\n",
    "            loss = criterion(outputs, labels)  # Compute loss\n",
    "            loss.backward()  # Backpropagation\n",
    "            optimizer.step()  # Update model weights\n",
    "            \n",
    "            running_loss += loss.item()  # Track loss for this batch\n",
    "    \n",
    "        # Validation phase (no gradients needed)\n",
    "        model.eval()  # Set model to evaluation mode\n",
    "        val_loss = 0.0\n",
    "        y_val_pred = []\n",
    "        y_val_true = []\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)  # Move to device\n",
    "                outputs = model(inputs)  # Get predictions\n",
    "                loss = criterion(outputs, labels)  # Compute loss\n",
    "                val_loss += loss.item()  # Track validation loss\n",
    "                y_val_pred.append(outputs.argmax(dim=1))  # Get predicted class labels\n",
    "                y_val_true.append(labels)  # True labels\n",
    "    \n",
    "        # Average losses\n",
    "        train_loss = running_loss / len(train_loader)\n",
    "        val_loss = val_loss / len(val_loader)\n",
    "    \n",
    "        # Convert validation predictions to a single tensor\n",
    "        y_val_pred = torch.cat(y_val_pred).cpu().numpy()\n",
    "        y_val_true = torch.cat(y_val_true).cpu().numpy()\n",
    "    \n",
    "        # Evaluate F1 score (or any other metric you prefer)\n",
    "        f1 = f1_score(y_val_true, y_val_pred, average='micro')\n",
    "        # print(f\"Epoch {epoch+1}/{epochs} - Train Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}, F1 Score: {f1:.4f}\")\n",
    "    \n",
    "        # Early stopping check\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), 'best_lstm_model.pth')  # Save the best model\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                # print(\"Early stopping triggered\")\n",
    "                break\n",
    "    \n",
    "    # Load the best model after training\n",
    "    model.load_state_dict(torch.load('best_lstm_model.pth'))\n",
    "    os.remove('best_lstm_model.pth')\n",
    "    \n",
    "    y_predicted = model(X_test_tensor)\n",
    "    probabilities = torch.softmax(y_predicted, dim=1)\n",
    "    y_predicted = torch.argmax(y_predicted, dim=1)\n",
    "    \n",
    "    results_temp = pd.DataFrame(y_predicted.cpu().detach().numpy(), \n",
    "                                index=y_test[num_lags*skip:].index, columns=['y_predicted'])\n",
    "    results_temp_probabilities = pd.DataFrame(probabilities.cpu().detach().numpy(),\n",
    "                                              index=y_test[num_lags*skip:].index)\n",
    "    return y_test[num_lags*skip:], results_temp, results_temp_probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "14947a51-a4a8-4353-9e27-6ab544493bc5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fully_combined_df = pd.concat([combined_df,df_shifts[week_out_str]],axis=1)\n",
    "fully_combined_df = fully_combined_df.dropna()\n",
    "dic_trainval, dic_test = get_train_val_test_periods(fully_combined_df)\n",
    "start_of_test_periods = np.arange(1981,2021,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d61215be-f1e0-4eca-be11-6c4742941741",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/glade/derecho/scratch/jhayron/tmp/ipykernel_174869/3939569735.py:115: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('best_lstm_model.pth'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/glade/derecho/scratch/jhayron/tmp/ipykernel_174869/3939569735.py:115: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('best_lstm_model.pth'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/glade/derecho/scratch/jhayron/tmp/ipykernel_174869/3939569735.py:115: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('best_lstm_model.pth'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/glade/derecho/scratch/jhayron/tmp/ipykernel_174869/3939569735.py:115: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('best_lstm_model.pth'))\n"
     ]
    }
   ],
   "source": [
    "list_results = []\n",
    "list_ytests_modified = []\n",
    "list_results_probs = []\n",
    "\n",
    "for iperiod in range(len(start_of_test_periods)):\n",
    "    print(iperiod)\n",
    "    X_trainval = dic_trainval[start_of_test_periods[iperiod]].iloc[:,:-1].values\n",
    "    y_trainval = dic_trainval[start_of_test_periods[iperiod]].iloc[:,-1]\n",
    "\n",
    "    X_test = dic_test[start_of_test_periods[iperiod]].iloc[:,:-1].values\n",
    "    y_test = dic_test[start_of_test_periods[iperiod]].iloc[:,-1]\n",
    "    \n",
    "    y_test_new, y_predicted, y_predicted_probs  = build_train_evaluate_lstm(X_trainval,y_trainval,\n",
    "                                                        X_test,y_test)\n",
    "    \n",
    "    df_results_temp = pd.DataFrame(np.array([y_test_new.values,y_predicted['y_predicted'].values]).T,\n",
    "                                   index=y_test_new.index,\n",
    "                                   columns=['y_true','y_predicted'])\n",
    "    list_results.append(df_results_temp)\n",
    "    list_results_probs.append(y_predicted_probs)\n",
    "    \n",
    "df_results_full = pd.concat(list_results,axis=0)\n",
    "df_results_full_probs = pd.concat(list_results_probs,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "bf3d5366-db45-431a-a7a5-2eb4d55d0347",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 LSTM: 0.22997542997542997\n",
      "F1 Random: 0.2176904176904177\n",
      "F1 Persistence: 0.2339066339066339\n",
      "F1 Climatology: 0.23267813267813267\n"
     ]
    }
   ],
   "source": [
    "f1_results = f1_score(df_results_full['y_true'],df_results_full['y_predicted'],average='micro')\n",
    "\n",
    "f1_random = f1_score(df_results_full['y_true'],\n",
    "         random_forecast['y_predicted'].loc[df_results_full['y_true'].index],average='micro')\n",
    "\n",
    "persistence_forecast = df_week_0.loc[df_results_full['y_true'].index]\n",
    "f1_persistence = f1_score(df_results_full['y_true'],\n",
    "         persistence_forecast['week0'].loc[df_results_full['y_true'].index],average='micro')\n",
    "\n",
    "f1_climatology = f1_score(df_results_full['y_true'],\n",
    "         climatology_forecast['y_predicted'].loc[df_results_full['y_true'].index],average='micro')\n",
    "print(f'F1 LSTM:', f1_results)\n",
    "print('F1 Random:',f1_random)\n",
    "print('F1 Persistence:',f1_persistence)\n",
    "print('F1 Climatology:',f1_climatology)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93124d9-874d-474a-bf06-1bd5a2d09d5c",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6a760848-1549-4102-8aeb-b4f96d16b8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "eeb525f5-0bd9-448b-adac-18b90debedb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_val_test_periods_cnn(X,y):\n",
    "    dic_train_val = {}\n",
    "    dic_test = {}\n",
    "    \n",
    "    start_of_test_periods = np.arange(1981,2021,10)\n",
    "    end_of_test_periods = start_of_test_periods + 9\n",
    "    \n",
    "    for iperiod in range(len(start_of_test_periods)):\n",
    "        X_test_temp = X.sel(time=slice(str(start_of_test_periods[iperiod]),str(end_of_test_periods[iperiod])))\n",
    "        X_trainval_temp = X.sel(time = ~X['time'].isin(X_test_temp.time))\n",
    "    \n",
    "        y_test_temp = y[str(start_of_test_periods[iperiod]):str(end_of_test_periods[iperiod])]\n",
    "        y_trainval_temp = y.drop(y_test_temp.index)\n",
    "        \n",
    "        dic_train_val[start_of_test_periods[iperiod]] = (X_trainval_temp,y_trainval_temp)\n",
    "        dic_test[start_of_test_periods[iperiod]] = (X_test_temp,y_test_temp)\n",
    "    return dic_train_val, dic_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ae763db7-f891-4bf0-9d33-c23a00d09c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CustomCNN(nn.Module):\n",
    "    def __init__(self, num_classes=5):\n",
    "        super(CustomCNN, self).__init__()\n",
    "        self.out_first_channel = 128\n",
    "        self.kernel_size_first_channel = 3\n",
    "        self.kernel_size_second_channel = 3\n",
    "        self.kernel_size_third_channel = 3\n",
    "        self.stride = 1\n",
    "        self.padding = \"valid\"\n",
    "        self.ks_maxpool = 4\n",
    "        self.stride_maxpool = 2\n",
    "        self.dropout_rate = 0.1\n",
    "        \n",
    "        # First convolutional block\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=self.out_first_channel, \n",
    "                               kernel_size=self.kernel_size_first_channel, stride=self.stride, padding=self.padding)\n",
    "        self.bn1 = nn.BatchNorm2d(self.out_first_channel)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=self.ks_maxpool, stride=self.stride_maxpool)\n",
    "        self.dropout1 = nn.Dropout(self.dropout_rate)\n",
    "        \n",
    "        # Second convolutional block\n",
    "        self.conv2 = nn.Conv2d(in_channels=self.out_first_channel, out_channels=self.out_first_channel*2, \n",
    "                               kernel_size=self.kernel_size_second_channel, stride=self.stride, padding=self.padding)\n",
    "        self.bn2 = nn.BatchNorm2d(self.out_first_channel*2)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=self.ks_maxpool, stride=self.stride_maxpool)\n",
    "        self.dropout2 = nn.Dropout(self.dropout_rate)\n",
    "        \n",
    "        # Third convolutional block\n",
    "        self.conv3 = nn.Conv2d(in_channels=self.out_first_channel*2, out_channels=self.out_first_channel*4, \n",
    "                               kernel_size=self.kernel_size_third_channel, stride=self.stride, padding=self.padding)\n",
    "        self.bn3 = nn.BatchNorm2d(self.out_first_channel*4)\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=self.ks_maxpool, stride=self.stride_maxpool)\n",
    "        self.dropout3 = nn.Dropout(self.dropout_rate)\n",
    "        \n",
    "        # Global average pooling layer\n",
    "        self.global_avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        \n",
    "        # Fully connected layer\n",
    "        self.fc1 = nn.Linear(self.out_first_channel*4, self.out_first_channel*2)\n",
    "        self.fc2 = nn.Linear(self.out_first_channel*2, self.out_first_channel)\n",
    "        self.fc3 = nn.Linear(self.out_first_channel, num_classes)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # First block: Conv -> BatchNorm -> ReLU -> MaxPool\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.dropout1(x)\n",
    "        \n",
    "        # Second block: Conv -> BatchNorm -> ReLU -> MaxPool\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool2(x)\n",
    "        x = self.dropout2(x)\n",
    "        \n",
    "        # Third block: Conv -> BatchNorm -> ReLU -> MaxPool\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool3(x)\n",
    "        x = self.dropout3(x)\n",
    "        \n",
    "        # Global average pooling\n",
    "        x = self.global_avg_pool(x)\n",
    "        \n",
    "        # Flatten for the fully connected layers\n",
    "        x = torch.flatten(x, 1)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f7478982-e1e2-4a69-ab31-e727160ca384",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_train_evaluate_cnn(X_trainval,y_trainval,X_test,y_test):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    X_trainval = X_trainval.expand_dims(\"channel\", axis=1)\n",
    "    X_test = X_test.expand_dims(\"channel\", axis=1)\n",
    "    \n",
    "    # Split into training and validation sets\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_trainval, y_trainval, test_size=0.2, shuffle=False)\n",
    "    \n",
    "    # Convert to PyTorch tensors\n",
    "    X_test_tensor = torch.tensor(X_test.values, dtype=torch.float32).to(device)\n",
    "    y_test_tensor = torch.tensor(y_test.values, dtype=torch.long).to(device)\n",
    "    \n",
    "    # Convert to PyTorch tensors\n",
    "    X_train_tensor = torch.tensor(X_train.values, dtype=torch.float32).to(device)\n",
    "    y_train_tensor = torch.tensor(y_train.values, dtype=torch.long).to(device)\n",
    "    X_val_tensor = torch.tensor(X_val.values, dtype=torch.float32).to(device)\n",
    "    y_val_tensor = torch.tensor(y_val.values, dtype=torch.long).to(device)\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "    \n",
    "    # Set random seed for reproducibility\n",
    "    torch.manual_seed(42)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(42)\n",
    "    \n",
    "    # Create data loaders\n",
    "    batch_size = 64\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Instantiate the model and move to the device\n",
    "    model = CustomCNN(num_classes=5).to(device)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()  # Suitable for multi-class classification\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "    \n",
    "    # Early stopping parameters\n",
    "    epochs = 50\n",
    "    patience = 5\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    \n",
    "    # Training loop with early stopping\n",
    "    for epoch in range(epochs):\n",
    "        model.train()  # Set model to training mode\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)  # Move to device (GPU/CPU)\n",
    "            \n",
    "            optimizer.zero_grad()  # Zero out the gradients\n",
    "            outputs = model(inputs)  # Get predictions from the model\n",
    "            loss = criterion(outputs, labels)  # Compute loss\n",
    "            loss.backward()  # Backpropagation\n",
    "            optimizer.step()  # Update model weights\n",
    "            \n",
    "            running_loss += loss.item()  # Track loss for this batch\n",
    "    \n",
    "        # Validation phase (no gradients needed)\n",
    "        model.eval()  # Set model to evaluation mode\n",
    "        val_loss = 0.0\n",
    "        y_val_pred = []\n",
    "        y_val_true = []\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)  # Move to device\n",
    "                outputs = model(inputs)  # Get predictions\n",
    "                loss = criterion(outputs, labels)  # Compute loss\n",
    "                val_loss += loss.item()  # Track validation loss\n",
    "                y_val_pred.append(outputs.argmax(dim=1))  # Get predicted class labels\n",
    "                y_val_true.append(labels)  # True labels\n",
    "    \n",
    "        # Average losses\n",
    "        train_loss = running_loss / len(train_loader)\n",
    "        val_loss = val_loss / len(val_loader)\n",
    "    \n",
    "        # Convert validation predictions to a single tensor\n",
    "        y_val_pred = torch.cat(y_val_pred).cpu().numpy()\n",
    "        y_val_true = torch.cat(y_val_true).cpu().numpy()\n",
    "    \n",
    "        # Evaluate F1 score (or any other metric you prefer)\n",
    "        f1 = f1_score(y_val_true, y_val_pred, average='micro')\n",
    "        # print(f\"Epoch {epoch+1}/{epochs} - Train Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}, F1 Score: {f1:.4f}\")\n",
    "        torch.save(model.state_dict(), 'best_cnn_model.pth')\n",
    "        # Early stopping check\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), 'best_cnn_model.pth')  # Save the best model\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                # print(\"Early stopping triggered\")\n",
    "                break\n",
    "    \n",
    "    # Load the best model after training\n",
    "    model.load_state_dict(torch.load('best_cnn_model.pth'))\n",
    "    os.remove('best_cnn_model.pth')\n",
    "\n",
    "    y_predicted = model(X_test_tensor)\n",
    "    probabilities = torch.softmax(y_predicted, dim=1)\n",
    "    y_predicted = torch.argmax(y_predicted, dim=1)\n",
    "    \n",
    "    results_temp = pd.DataFrame(y_predicted.cpu().detach().numpy(), index=y_test.index, columns=['y_predicted'])\n",
    "    results_temp_probabilities = pd.DataFrame(probabilities.cpu().detach().numpy(),\n",
    "                                              index=y_test.index)\n",
    "    return results_temp, results_temp_probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "db1e3054-096f-4280-8d14-d3c5f037675f",
   "metadata": {},
   "outputs": [],
   "source": [
    "timeanoms = anoms.time.values\n",
    "timewrs = df_shifts[week_out_str].dropna().index.values\n",
    "time_intersection = np.intersect1d(timeanoms, timewrs)\n",
    "\n",
    "X = anoms[var_name_nc].sel(time=time_intersection)\n",
    "y = df_shifts[week_out_str].loc[time_intersection]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8bbe34db-b363-4cdc-9bce-8011a4cd0c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_trainval, dic_test = get_train_val_test_periods_cnn(X,y)\n",
    "start_of_test_periods = np.arange(1981,2021,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f11c1da9-b2a6-44f2-9091-fd36f2eb65c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/glade/derecho/scratch/jhayron/tmp/ipykernel_174869/3032166677.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('best_cnn_model.pth'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/glade/derecho/scratch/jhayron/tmp/ipykernel_174869/3032166677.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('best_cnn_model.pth'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/glade/derecho/scratch/jhayron/tmp/ipykernel_174869/3032166677.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('best_cnn_model.pth'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/glade/derecho/scratch/jhayron/tmp/ipykernel_174869/3032166677.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('best_cnn_model.pth'))\n"
     ]
    }
   ],
   "source": [
    "list_results = []\n",
    "list_results_probs = []\n",
    "\n",
    "for iperiod in range(len(start_of_test_periods)):\n",
    "    print(iperiod)\n",
    "    X_trainval = dic_trainval[start_of_test_periods[iperiod]][0]\n",
    "    y_trainval = dic_trainval[start_of_test_periods[iperiod]][1]\n",
    "\n",
    "    X_test = dic_test[start_of_test_periods[iperiod]][0]\n",
    "    y_test = dic_test[start_of_test_periods[iperiod]][1]\n",
    "\n",
    "    y_predicted, y_predicted_probs = build_train_evaluate_cnn(X_trainval,y_trainval,X_test,y_test)\n",
    "    \n",
    "    df_results_temp = pd.DataFrame(np.array([y_test.values,y_predicted['y_predicted'].values]).T,\n",
    "                                   index=y_test.index,\n",
    "                                   columns=['y_true','y_predicted'])\n",
    "    \n",
    "    list_results.append(df_results_temp)\n",
    "    list_results_probs.append(y_predicted_probs)\n",
    "    \n",
    "    # Collect garbage to release memory\n",
    "    gc.collect()\n",
    "    \n",
    "    # Clear PyTorch cache\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "df_results_full = pd.concat(list_results,axis=0)\n",
    "df_results_full_probs = pd.concat(list_results_probs,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0fb37da4-ffa8-47e5-b6c9-9189251f57f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 CNN: 0.2340009770395701\n",
      "F1 Random: 0.21812408402540304\n",
      "F1 Persistence: 0.23326819736199317\n",
      "F1 Climatology: 0.23277967757694187\n"
     ]
    }
   ],
   "source": [
    "f1_results = f1_score(df_results_full['y_true'],df_results_full['y_predicted'],average='micro')\n",
    "\n",
    "f1_random = f1_score(df_results_full['y_true'],\n",
    "         random_forecast['y_predicted'].loc[df_results_full['y_true'].index],average='micro')\n",
    "\n",
    "persistence_forecast = df_week_0.loc[df_results_full['y_true'].index]\n",
    "f1_persistence = f1_score(df_results_full['y_true'],\n",
    "         persistence_forecast['week0'].loc[df_results_full['y_true'].index],average='micro')\n",
    "\n",
    "f1_climatology = f1_score(df_results_full['y_true'],\n",
    "         climatology_forecast['y_predicted'].loc[df_results_full['y_true'].index],average='micro')\n",
    "print(f'F1 CNN:', f1_results)\n",
    "print('F1 Random:',f1_random)\n",
    "print('F1 Persistence:',f1_persistence)\n",
    "print('F1 Climatology:',f1_climatology)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f778a59d-78dd-4837-b63a-ae8a28f0cfc3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch_wr]",
   "language": "python",
   "name": "conda-env-pytorch_wr-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
